---
title: "Assessing the Replicability of Psychological Science Through Pedagogy"
short-title: "Replication Through Pedagogy"
output: kmr::apa_manuscript

csl: apa6.csl
bibliography: replication254.bib

document-params: "a4paper,man,apacite,floatsintext,longtable"

bib-tex: "replication254.bib"

author-information:
    - \author{Eric N. Smith*, Robert X. D. Hawkins*, the students of Psych 254**, and Michael C. Frank}

affiliation-information:
    - \affiliation{Department of Psychology, Stanford University}

author-note:
    "*These authors contributed equally. **Project contributors included Carolyn Au, Juan Arias, Rhia Catapano, Eric Hermann, Marty Keil, Andrew Lampien, Sarah Raposo, Jesse Reynolds, Shima Salehi, Justin Salloum, and Jed Tan. Thanks to the Stanford Department of Psychology and the Vice Provost for Graduate Education for funding to support the class."
    
abstract: 
    "Replications are important to science, but who will do them? One proposal is that students can conduct direct replications as part of their training. As a proof-of-concept for this idea, here we report a series of 11 pre-registered, direct replications of findings from the 2015 volume of Psychological Science, all conducted as part of a graduate-level course. This work provides an estimate of the probability that a motivated graduate student can reproduce a previously published finding of interest on a first attempt. Congruent with previous findings, replication studies typically yielded smaller effects than originals: The modal outcome was partial support for the original claim. We describe the workflow and pedagogical methods that were used in the class and discuss challenges for adoption of this pedagogical model and replication research more broadly."
    
keywords:
    "Replication; Reproducibility; Pedagogy; Experimental Methods"
---

```{r global_options, include=FALSE}
rm(list=ls())
library(knitr)
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = FALSE, 
                      fig.path='figs/', echo=FALSE, warning = FALSE, 
                      cache=TRUE, message = FALSE, sanitize = TRUE)
library(readr)
library(dplyr)
library(xtable)
library(readr)
library(tidyr)
library(ggplot2)
library(BayesFactor)
library(ggrepel)
#devtools::install_github("langcog/langcog")
library(langcog)
library(cowplot)
library(png)
```

Replicability is a core value for empirical research and there is increasing concern throughout psychology that more independent replication is necessary [@osc2015;@wagenmakers2012]. Yet under the current incentive structure for science, direct replication is not typically valued. One potential solution to this problem is to make direct replication an explicit part of pedagogy: that is, to teach students about experimental methods by asking them to run replication studies [@frank2012;@grahe2012]. Despite enthusiasm for this idea [@lebel2015;@everett2015;@standing2016;@king2016], there is limited data beyond anecdotal reports or individual projects [@phillips2015;@lakens2013] to support its efficacy. 

In the current article, we provide evidence on this issue by reporting the results of replication projects conducted in a single class (a graduate-level experimental methods course). As part of the required work of the course, students conducted high-power, direct replications of published articles from the 2015 volume of the journal *Psychological Science*. These studies both give evidence about the state of replicability in the field and provide insight into the process of pedagogical replication. 

It is a challenging proposition to estimate the replicability of an entire literature [cf. @osc2015]. Making such an estimate minimally requires determining a sampling strategy, a notion of what constitutes a direct replication, and a standard for what constitutes a successful replication, among many other difficult decisions [@gilbert2016;@anderson2016]. In the current work, we focus on a quantity that is easier to estimate: the probability that a graduate student can choose an article of interest in *Psychological Science* and -- in a single attempt, within constraints of budget, expertise, and effort -- reproduce the basic pattern of findings with sufficient fidelity to be able to build on it in future work. We believe that this notion of whether a piece of research supports cumulative progress is an important construct. Whether an independent scientist can build on a published result is perhaps as important as whether the work is replicable in a more conceptual sense -- and in many cases easier to determine. 

With respect to pedagogy, we describe our process for conducting well-powered, high-quality, direct replications as part of classroom pedagogy so that other institutions may adapt these practices. This process is dramatically facilitated by the use of free standardized tools for statistical analysis (R), data and code sharing (Git, R Markdown), and creation of web experiments (HTML, CSS, JavaScript). Nevertheless, there are significant limitations on what can be done in a single term and within the constraints of a course budget. We return in the Discussion to limitations on our approach. 

In sum, we view the contribution of the current work to be two-fold. First, we provide a proof-of-concept and pedagogical model for conducting replications in the classroom. Second, we provide an initial estimate of what proportion of studies from the most recent volume of *Psychological Science* could be reproduced -- supporting the same theoretical claims -- by a motivated student. 

```{r, results='asis'}
project_plan_raw <- read.csv("../data/planning_and_outcomes.csv")

project_plan <- project_plan_raw %>%
  filter(`include_in_final_writeup`==1) 

project_data_summary <- read_csv("../data/project_data_summary.csv")

d <- left_join(project_plan, project_data_summary, by = c("project_key")) %>%
  rowwise() %>%
  mutate(fidelity = (replication_fidelity_MCF + replication_fidelity_ES) / 2) %>%
  ungroup
```

```{r}
ppt <- project_plan %>%
        select(cite, expt_num, open_data,
               open_materials, original_on_turk, original_n,collected_n)

names(ppt) = c("Citation", "Expt", "Data", "Materials", "Turk?", 
               "N (orig)","N (rep)")

# print(xtable(ppt, caption = "Summary of attempted replications", 
#        label = "tab:summary"), include.rownames=FALSE)
```

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
  
<!-- latex table generated in R 3.3.0 by xtable 1.8-2 package
 Fri Jun 24 12:02:11 2016 -->
\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{lllllrr}
  \hline
Citation & Expt & \specialcell{Open\\Data?} & \specialcell{Open\\Materials?} & \specialcell{Original\\on Turk?} & N (orig) & N (rep) \\ 
  \hline
Storm \& Stone (2015) & 3 & No & No & No &  48 &  61 \\ 
Lewis \& Oyserman (2015) & 4 & No & Yes & Yes & 122 & 128 \\ 
Scopelliti, Loewenstein, \& Vosgerau (2015) & 3 & No & Yes & Yes & 550 & 124 \\ 
Liverence \& Scholl (2015) & 1 & No & Some & No &  18 &  19 \\ 
Wang et al. (2015) & 2 & No & No & No & 219 & 397 \\ 
Sofer et al. (2015) & 1 & No & Yes & No &  48 &  95 \\ 
Ko, Sadler, \& Galinksy (2015) & 2 & Yes & Some & No &  40 &  40 \\ 
Atir, Rosenzweig, \& Dunning (2015) & 1b & No & Yes & Yes & 202 &  50 \\ 
Proudfoot, Kay, \& Koval (2015) & 1 & No & No & Yes &  80 &  84 \\ 
Zaval, Markowitz, \& Weber (2015) & 1 & Yes & Yes & Yes & 312 & 321 \\ 
Xu \& Franconeri (2015) & 1a & No & No & No &  12 &  27 \\ 
   \hline
\end{tabular}
\caption{\label{tab:summary} Summary of papers and experiments that were targets for replication in our projects.} 
\label{tab:summary}
\end{table}

# Methods

We begin by describing the general method of the class, then discuss shared methods for the replications and the process of development for each project.

## Study Selection

All projects were completed as part of a graduate-level methodology class (syllabus and materials available at [http://psych254.stanford.edu]()). At the initiation of class, all students were told that they had the opportunity to contribute their individual class assignment to a group replication project. The requirements for joining the project were to conduct a pre-registered replication of a finding from the 2015 volume of *Psychological Science* and to contribute the code, data, and materials to the writeup. 

Students selected projects based on interest, rather than via a systematic sampling strategy. After selecting a paper of interest, students chose a particular study from within that paper based on their judgment of interest and feasibility. The modal study chosen was the first, but several students chose later studies in a set as well.  Thus, all interpretation of our findings should be with respect to the distribution of student interest. Any reproducibility estimates arising from our analyses are estimates of the probability of reproducing a finding that is of interest, rather than findings in general. This sampling strategy may thus bias our results towards more exciting or novel findings, as well as those that were more feasible for students. A summary of the original experiments chosen for replication is given in Table \ref{tab:summary}. 

Students chose findings from a wide variety of domains for replication. One cluster of studies included investigations of memory and visual attention [e.g., @xu2015], for example in applied contexts like remembering "deleted" files [@storm2015] or tracking transition between locations on smartphone apps [@liverence2015]. A second cluster came from studies of social perception and judgment, including judgments of faces and voices [@ko2015;@sofer2015], as well as studies of attributions of modesty, creativity, and expertise [@atir2015;@scopelliti2015;@proudfoot2015]. Other studies investigated increasing retirement savings by orienting participants to the future [@lewis2015], legacy priming [@zaval2015], and the effects of math anxiety on performance [@wang2015]. Details of individual replications are available in SOM. 

## Participants

All study participants were recruited on Amazon Mechanical Turk; individual samples are given in Table \ref{tab:summary}.^[Note that of the `r nrow(ppt)` studies included in the final sample, `r sum(project_plan$original_on_turk=="Yes")` (`r 100 * round(mean(project_plan$original_on_turk=="Yes"), digits=2)`%) were originally conducted on Mechanical Turk.] All experiments were approved by the Stanford University institutional review board under protocol \#23274, "Reproducibility of psychological science and instruction."

Determining appropriate sample sizes is a major challenge in replication research [@button2013;@simonsohn2015]. Post-hoc power analyses are problematic because of the likelihood of inflation of effect sizes due to the "winner's curse" [@hoenig2001;@button2013], but -- especially within the constraints of a limited budget -- it can be impossible to follow more conservative guidelines in all cases. For example, @simonsohn2015 recommends 2.5x the original sample, which can be feasible for small or under-powered original studies but may lead to impractical or unnecessary recommendations for experiments that were initially large and/or adequately powered. 

We eventually used a hybrid, case-by-case selection criterion that attempted to maximize the success of the project while staying within the constraints of our budget.
^[We initially were allocated \$1,500 but one student contributed personal research funds, leading to a total cost of $\approx$ \$1,700 for all studies. We attempted to set payment for our studies at approximately \$6/hour based on timing estimated from Pilot A sessions.] 
In 
`r sum(project_plan$power_standard == "80% power")`
cases, we powered the replication attempt to 80\% post-hoc power. In 
`r sum(project_plan$power_standard == "Original")` 
cases, we used the original sample size. Three replications deviated from these two criteria.  In one case, the original experiment was extremely over-powered in the post-hoc analysis, so we chose a smaller sample size that still would yield high power.  In another, we reduced the number of trials as part of the adaptation to Mechanical Turk. In the last case of a multi-step procedure where the first step consisted of stimulus generation, we decreased the N in the first stage of the procedure (details available in SOM). 

## Materials and Methods

Of the `r nrow(ppt)` studies included in the final sample, `r 100 * round(mean(project_plan$open_materials == "Yes" | project_plan$open_materials == "Some"), digits=2)`% of studies had either some or all materials openly available. For `r sum(project_plan$author_contact != "")` studies, however, students contacted the authors to request either materials or clarification of methods (using a template email that was customized by the students and reviewed by the instructors). Responses were received in all but one case, all within a matter of days.  

## Workflow

All materials and methods for our replication studies are available at [https://github.com/StanfordPsych254](). All experiments were coded in JavaScript, HTML, and CSS so as to be run in a standard web browser. All analyses were written using R, a free and open-source platform for statistical analysis. A schematic of our class workflow is shown in Figure 1. 

```{r, fig.cap="A schematic view of our class timeline for replication projects. R indicates the approximate timing of instructor team reviews of student materials.", fig.height=4, fig.width=6, fig.align="center", fig.pos="t"}

img <- readPNG(source = "../figures/pipeline.png")
plot.new()
plot.window(0:1, 0:1)

#fill plot with image
usr<-par("usr")    
rasterImage(img, usr[1], usr[3], usr[2], usr[4])
```

Students wrote their final reports using [R Markdown](http://rmarkdown.rstudio.com/), a "literate programming" environment in which statistical analysis code can be interspersed with text, figures, and tables. This part of our workflow was extremely useful both pedagogically and for encouraging reproducible research practices. Using the template developed by @osc2015 for replication reports, students created dynamic documents that included their proposed replication study, the code necessary to analyze their data, and the outputs of their data analysis (e.g. statistical tests, figures). This method of writing allows students to share a single compiled document via a hyperlink, facilitating review of writing, results, and code together in a single platform.^[The current manuscript is written in this fashion as well.] This writing method is also likely to reduce the frequency of statistical reporting errors [which appear to be regrettably common; @nuijten2015], given that many of these are likely introduced by transferring results between analysis outputs and manuscripts. 

## Preregistration and Review

In addition to as-needed guidance on projects, to ensure high quality replications, students went through a rigorous process in which each student's work was reviewed several times by both the instructor (MCF) and the teaching assistants (ENS and RXDH). Each student collected two pilot samples and review was coordinated with these. The first, "Pilot A," consisted of a handful of non-naive participants (e.g., the experimenter, other students). The goal of this pilot was to ensure that all needed data were being logged by the experiment script and that analytic scripts for the confirmatory analyses functioned appropriately. After Pilot A was completed, the instructor or a TA critiqued and reviewed the student's experimental script (acting as a participant), analytic code, and resulting output. 

Once requested changes were made, the student conducted "Pilot B," using a handful of naive participants recruited from Mechanical Turk. The goal of this pilot was to ensure that participants were able to complete the experiment and did not report any substantial issues with instructions or technical details of the experiment. (All students were instructed to give participants a way to leave free-form comments at the end of the experiment but prior to debriefing text). At the conclusion of Pilot B, both the instructor and a TA reviewed the student's analytic code and its outputs on the data for all confirmatory analyses. The goal of this code review was to ensure that all planned analyses were specified correctly and with sufficient detail to permit inclusion in the broader analysis.

After Pilot B review was completed, students were given authorization to collect the full sample of participants, contingent on having made any requested changes. Prior to data collection, the analytic script containing all confirmatory analyses was pre-registered using the Open Science Framework. In addition, prior to data collection, we pre-registered the confirmatory analyses reported in the current paper ([https://osf.io/rxz8m/](https://osf.io/rxz8m/)).

## Statistical approach

Despite the general emphasis in meta-analytic and reproducibility efforts to aggregate a single effect size of interest [@osc2015;@lipsey2001], it was often challenging to identify a single key statistical test in studies. Often a wide variety of statistical tests were conducted, often within one or several regression models with multiple specifications. Thus, we interpret "key statistical test" results with caution. Nevertheless, despite this interpretive difficulty, we attempted to use a single statistical test as our target for experiment planning and analysis.

# Results

## Pedagogical assessment 

Of the 15 students in the class, all completed projects (two with extensions going beyond the class period), one chose a project outside of the sampling frame, and three opted out of the broader project prior to data collection.^[Note that these students did not express doubts about the success of their projects prior to opting out, and 2/3 of these projects were judged by the instructor team to have been successful replications. Thus we have no evidence that these students opted out systematically due to their belief that their project would fail.] The remaining 11 students contributed code, data, registrations and materials to the final product. (In one case, a student did not complete the pre-registration procedure correctly but nevertheless submitted a pre-specified analysis plan to the instructors for review.  Thus, this project remains included.) 

Each member of the instructor team coded the fidelity of replications, weighing whether the materials, sample, and task parameters differed from the original studies. Coded on a scale of 1 -- 7 (with 1 being a loose replication with substantive deviations from the original, and 7 being essentially identical), the mean rating was `r round(mean(d$fidelity), digits = 2)` (Range = `r round(min(d$fidelity), digits = 2)` to `r round(max(d$fidelity), digits = 2)`). Several studies were essentially identical to the originals, but there was a group of others that included differences in population or number of trials that might have an effect on the results. 

In two cases, in final review of the projects (after data collection), the instructor team decided that the student's choice of key statistical test was not in fact a strong test of the original authors' primary theoretical claim. In both of these cases [@sofer2015;@proudfoot2015], the original authors had fit a multiple regression model to the data and there was not a single obvious test that corresponded directly to the authors' hypothesis. After discussion, the instructor team converged on a statistical test that they thought better corresponded to the original authors' intended hypothesis of interest. We report results from these corrected tests here. We return to this issue in the Discussion, as we believe that test selection is a critical theoretical issue in replication research. 

## Confirmatory analyses

### Subjective judgments of replication


100 * round(mean(d$replicated_TA[d$high_fidelity == FALSE]), digits=2)`\%


Students of the class and the instructors gave independent “replication ratings” to assess subjective judgement of the replication. Our three-point rating system was based on the theoretical interpretation of the original finding, with 0\% = no support for the original interpretation, 50\% = some support for the original interpretation, 100\% = full replication). We found that the ratings of the student carrying out the replication and the instructors were in agreement for `r sum(d$replicated_TA == d$replicated_student,na.rm=T)` out of the `r nrow(d)` replications.  The average rating given by instructors and students was `r 100 * round(mean(d$replicated_TA, na.rm=TRUE), digits = 2)`\% and `r 100 * round(mean(d$replicated_student, na.rm=TRUE), digits = 2)`\% respectively.  

```{r}
d$high_fidelity <- d$fidelity > median(d$fidelity)
# table(d$replicated_TA,d$replicated_student) # Make prettier
#mean(d$replicated_team)

replicated <- d %>% 
  group_by(replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  mutate(measure = "replicated",
         value = "")

turk <- d %>% 
  group_by(original_on_turk, replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  rename(value = original_on_turk) %>%
  mutate(measure = "original on turk") 

fidelity <- d %>% 
  group_by(high_fidelity,replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  mutate(measure = "high fidelity") %>%
  rename(value = high_fidelity) %>%
  ungroup() %>%
  mutate(value = c("Yes","No")[as.numeric(value) + 1])

open_materials <- d %>% 
  group_by(open_materials,replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  mutate(measure = "open materials") %>%
  rename(value = open_materials)

outcomes <- bind_rows(replicated, turk, fidelity, open_materials) 
names(outcomes) <- c("No","Partial","Full", "Mediator","Value")
outcomes <- outcomes %>%
  select(Mediator, Value, No, Partial, Full)

# print(xtable(outcomes), include.rownames = FALSE)
```

\begin{table}[ht]
\centering
\begin{tabular}{rl|ccccccc}
  \hline
\multicolumn{2}{c}{Support} & \multicolumn{2}{c}{Original on Turk?} & \multicolumn{2}{c}{High fidelity?} & \multicolumn{3}{c}{Open Materials?} \\
  \hline
   &  & No & Yes & Yes & No & No & Some & Yes \\ 
  Full & 3 & 1 & 2 & 1 & 2 & 1 & 1 & 1 \\ 
  Partial & 6 & 4 & 2 & 4 & 2 & 2 & 1 & 3 \\ 
  No &  2 &  1 &  1 &  2 & 0 &  1 & 0 &  1 \\ 
  \hline
  Total & 11 & 6 & 5 & 7 & 4 & 4 & 2 & 5 \\
   \hline
\end{tabular}
\caption{\label{tab:results} Counts of instructor team ratings of replication support for the theoretical findings, split by variables of interest. High fidelity denotes replications judged to be above the median rating of fidelity ( >`r median(d$fidelity)` out of 7).}
\end{table}

<!-- \begin{table}[ht] -->
<!-- \centering -->
<!-- \begin{tabular}{llrrr|r} -->
<!--   \hline -->
<!-- Mediator & Value & No & Partial & Full & Total\\  -->
<!--   \hline -->
<!-- Replicated? &  &   2 &   6 &   3 & 11 \\  -->
<!-- \hline -->
<!--   Original on turk & Yes &   1 &   2 &   2 & 5 \\  -->
<!--    & No &   1 &   4 &   1 & 6\\  -->
<!-- \hline -->
<!--   High fidelity replication? & Yes &   2 &   3 &   1  & 6\\  -->
<!--    & No &  &   3 &   2 & 5\\  -->
<!-- \hline -->
<!--   Open materials? & Yes &   1 &   3 &   1  & 4\\  -->
<!--    & Some &  &   1 &   1 & 2\\  -->
<!--    &  No &   1 &   2 &   1 & 4\\  -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \caption{\label{tab:results} Counts of instructor team replication ratings, split by various different mediating variables.} -->
<!-- \end{table} -->

The modal replication project in our sample was judged to be a "partial" replication, meaning that some aspects of the observed findings were different from those reported by the original authors. While this conclusion is dispiriting, it is also perhaps unsurprising given the complexity of the analyses reported in many of the original papers. In addition, `r sum(d$original_test=="interaction")` of the tests were interactions, which tend to have lower statistical power [@mcclelland1993] and have fared poorly in previous replication studies [@osc2015].

The distribution of ratings are given in Table \ref{tab:results}, along with breakdowns by various factors of interest. Since our sample only includes 11 studies, we describe general trends here rather than attempting to conduct statistical tests (which would be dramatically under-powered after correcting for multiple comparisons). Overall, projects had a somewhat larger probability of being judged successful if they were judged to be closer to the original (`r 100 * round(mean(d$replicated_TA[d$high_fidelity]), digits=2)`\% 
vs. 
`r 100 * round(mean(d$replicated_TA[d$high_fidelity == FALSE]), digits=2)`\%).
There were small numerical effects of being run on Mechanical Turk (`r 100 *round(mean(d$replicated_TA[d$original_on_turk == "Yes"]), digits=2)`\% vs. `r 100 *round(mean(d$replicated_TA[d$original_on_turk == "No"]), digits=2)`\%) and having open materials (`r 100 *round(mean(d$replicated_TA[d$open_materials!="No"]), digits=2)`\% 
vs. `r 100 *round(mean(d$replicated_TA[d$open_materials=="No"]), digits=2)`\%), though we do not believe these differences are interpretable given our sample size. 

### Significance of the key effect 

```{r}
d$original_p_value_bins <- .1
d$original_p_value_bins[d$original_p_value %in% c("<.05","0.04")] <- "<.05"
d$original_p_value_bins[d$original_p_value %in% c("0.007","0.001")] <- "<.01"
d$original_p_value_bins[d$original_p_value %in% c("<.001","< .001")] <- "<.001"
d$rep_p_value_05 <- d$rep_p_value < .05

d$replicated_TA_factor <- factor(d$replicated_TA, levels = c(0, .5, 1), 
                                 labels = c("No","Partial","Yes"))
# ggplot(d,aes(x=original_p_value_bins,y=rep_p_value, 
#              color=d$replicated_TA_factor)) +
#   geom_jitter(height=0, width=.2) +
#   geom_hline(yintercept=c(0.05), lty = 2) + 
#   geom_hline(yintercept=c(0.1), lty = 3) + 
#   ylim(c(0,1)) +
#   xlab("Original p-value (binned)") + 
#   ylab("Replication p-value (exact)") + 
#   theme_bw() + 
#   scale_color_solarized(name = "Subjective replication rating") + 
#   theme(legend.position = "bottom")
```

We next turn to an assessment of the replication $p$-value to determine whether the key statistic found a significant effect at the traditional $p=.05$ level. Only `r sum(d$rep_p_value_05)` of the studies yielded a significant replication $p$-value. The well-documented "dance of the p-values" [@cumming2013] suggests that the inference drawn from a single replication p-value may not be  informative, however, and other metrics such as effect sizes and Bayes factors may provide a more nuanced perspective. 

### Effect size and Bayes factor analysis

Effect sizes and Bayes factors improve upon inferences drawn from $p$-values in different ways. The effect size is an estimate of the *magnitude* of an effect, which is stable across sample sizes and allows for a more fine-grained comparison across studies than a binary significance criterion. The Bayes Factor is an alternative hypothesis testing method that quantifies the *evidence* in favor of a hypothesis [@jeffreys1961]. Among other advantages, it does not privilege the null hypothesis and can provide evidence either *for* or *against* [see also @scheibehenne2016;@wagenmakers2016].  While the Bayes Factor often agrees with the $p$-value as to which hypothesis is more likely [@wetzels2011], they often substantially disagree on the strength of the evidence.

These additional measures therefore give another sense in which replications can be successful: if we find roughly the same effect size or Bayes factor despite $p$-values on different sides of the $p = 0.05$ threshold, we should have increased confidence in the effect's reproducibility. To compute a standardized effect size, we followed @osc2015 in converting test statistics to a measure of "correlation coefficient per df," which is bounded between 0 and 1^[For example, the conversion from a $t$ statistic is given by $r = \sqrt{t^2/(t^2 + df)}$, where $df$ is the degrees of freedom. Other formulas are given in @osc2015, Supplemental Information.]. We first compared effect sizes from the original and replication studies (Figure 2A). Effect sizes were highly correlated between the two sets of studies 
(r = `r signif(cor.test(d$original_ES_d,d$rep_es)$estimate, digits =2)`, p < .0001), but they were generally smaller in the replications 
(`r signif(coef(lm(d$rep_es~d$original_ES_d-1))[1], digits = 2)*100`\% of the original; 95\% CI [`r signif(confint(lm(d$rep_es~d$original_ES_d-1))[1], digits = 2)*100` - `r signif(confint(lm(d$rep_es~d$original_ES_d-1))[2], digits = 2)*100`]). 

While we preregistered an analysis determining whether the original reported effect size is included in the 95\% confidence interval found in the replication, we instead opted for a straightforward comparison of point estimates. This choice was motivated by recent concerns that using confidence intervals in this manner may lead to misleading interpretations [@morey2016], as well as discrepencies in the authors' reporting of effect sizes across the articles we included (which made computing confidence intervals difficult in some cases). 

Next, following @etz2016, we compared Bayes Factors between the original and replication. We used the default  test suggested by @rouder2009 and did not attempt to correct for publication bias. This analysis revealed a number of interesting findings. First, as with effect sizes, we saw generally smaller Bayes Factors for the replications than the originals. Second, it appeared that replication Bayes Factors generally tracked nicely with subjective replication judgments as well. Finally, we note that the Bayes Factors for a number of the original effects did not appear to show strong evidential value (e.g. BF < 3); thus, it would have been somewhat surprising for our replications to show stronger evidence. 

```{r, fig.cap="Replication effect size (A) and Bayes factor (B), plotted by the original effect size and Bayes factor, respectively. Point size shows replication N (A) and ratio of test degrees of freedom (B), color indicates subjective replication assessments by the authors. Note that the key statistic in the Sofer et al. (2015) replication was a multivariate $F$ test, hence a default Bayes factor could not be computed.", fig.pos="ht", fig.height=4}

p1 <- ggplot(d,aes(x=original_ES_d,y=rep_es)) +
  geom_point(alpha = .8, 
             aes(color=replicated_TA_factor, size = collected_n)) +
  geom_text_repel(aes(label = first_author_name), size = 2, force = 2) +
  scale_x_continuous(limits = c(0, 1), 
                breaks = c(.1, .3, .5, .7, .9)) +
  scale_y_continuous(limits = c(0, 1), 
                breaks = c(.1, .3, .5, .7, .9)) +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  geom_smooth(method = "lm", se=FALSE) +
  xlab("Orig. effect size (d)") + 
  ylab("Rep. effect size (d)") + 
  theme_bw(base_size=10) + theme(legend.position="bottom") + 
  scale_size(name = "Rep. N") +
  # scale_color_solarized(guide = FALSE)
  scale_color_solarized(name = "Subjective rep. rating") 

getBF <- function(t_stat, t_df) {
  return(ttest.tstat(t = t_stat, n1 = t_df, rscale = 1, simple = T))
};

tstatDF <- d %>%
  filter(original_t_stat != "NA") %>%
  select(project_key, replicated_TA_factor, first_author_name, original_t_stat, original_t_df, rep_t_stat, rep_t_df) %>%
  rowwise() %>%
  mutate(origBF = getBF(original_t_stat, original_t_df)) %>%
  rowwise() %>%
  mutate(repBF = getBF(rep_t_stat, rep_t_df), 
         df_ratio = rep_t_df/original_t_df)

tstatDF$origBF[7] <- 1e+11

p2 <- ggplot(tstatDF, aes(x = origBF, y = repBF)) +
  geom_point(alpha = .8, aes(size = df_ratio, 
                             col = replicated_TA_factor)) +
  geom_abline(slope = 1, lty = 2) + 
  geom_smooth(method = "lm", se=FALSE) +
  geom_text_repel(aes(label = first_author_name), size = 2, force = 2) +
  xlab("Orig. Bayes Factor") + 
  ylab("Rep. Bayes Factor") + 
  scale_x_log10(limits = c(10**-1.75, 10**11), 
                breaks = c(1/10, 1, 10, 100, 1e+3, 1e+6, 1e+9),
                labels = c(".1", "1", "10","100", "1e3", 
                           "1e6","1e9")) +
  scale_y_log10(limits = c(10**-1.75, 10**11),
                breaks = c(1/10, 1, 10, 100, 1e+3, 1e+6, 1e+9),
                labels = c(".1", "1", "10","100", "1e3", 
                           "1e6","1e9")) + 
  theme_bw(base_size=10) + theme(legend.position="bottom") + 
  scale_size_continuous(name = "Ratio of rep. df to orig. df") + 
  scale_color_solarized(name = "Subjective rep. rating")

plot_grid(p1, p2, labels = c("A", "B"))
```



<!-- Demographics -->




# Discussion

In this paper, we reported the results of a series of 11 direct replications of previously-published experiments from the 2015 volume of *Psychological Science*. We had two interlocking goals: first, to create a proof of concept for high-quality pedagogical replications, and second, to provide evidence on the ability of students replicate important, recent findings in a top journal. Despite our relatively tight adherence to the original experimental protocols and relatively large sample sizes, our results were underwhelming and further suggests the need for more replication attempts as a means for cumulative scientific progress. The modal outcome of our projects was partial replication, in which some hint of the original pattern was observed but the key statistical test was not statistically significant and showed both smaller effect size and lower evidential value than the same test in the original. 

Our project aimed to assess the probability that a finding -- selected on grounds of feasibility and interest -- would be replicated with the time, money, and effort constraints of a course project. While these constraints might initially seem severe, our work here shows what is possible for motivated students using freely-available tools and resources. Indeed, the majority of the projects in our report were relatively close replications and included the same or greater numbers of participants. Nevertheless, they had a number of limitations, including at least: 1) limited time for iterative piloting and adjustment [@lewis2016], 2) limits on funding for samples, and 3) limits on domain expertise with respect to the specific effects that were chosen. In addition, many papers published within our notional sampling frame (the 2015 volume of *Psychological Science*) were not feasible as projects, and students in our course were asked to take feasibility into account in choosing their projects as well as their own interests. 

Our findings were congruent with the results of @osc2015, revealing a relatively low rate of replicability by a number of tests. In addition, in informal assessments of previous years' findings, we see a similar rate of replicability [@frank2015]. What are the causes of this relatively low rate? 

Many statistical factors leading to lowered replicability have been discussed in past work, including analytic flexibility, publication bias, and low statistical power among others [e.g., @button2013;  @ioannidis2005]. On the other hand, an alternative viewpoint is that -- especially for studies that leverage social constructs -- exact replication may be difficult across contexts [@gilbert2016;@vanbavel2016]. For both reasons of design and scale, our study here cannot disentangle these factors empirically. Nevertheless, the consistent decreases in effect size and evidential value we report seem consistent with a "winner's curse" -- that initial publications tend to over-estimate effects. However, in addition, although our sample was too small for statistical inference, numerically our results support replication fidelity in terms of sample and procedure as having some effects on success.

Our experiences performing and compiling the studies here sheds light on one further concern that we believe has been under-reported in past studies [including @osc2015]. Our standard model of replication is based on the notion of a single statistical test -- and its associated effect size -- being the key properties of a study that can be targeted for replication. Yet, as we mentioned above, selecting this key statistical test was difficult for most of the projects in our sample, and virtually impossible for some. We were often forced to consult other sections of the papers (e.g., abstracts, discussion sections) to gain clarity on what test was considered the critical one for the authors' interpretation. It is likely that some of our decisions would not be ratified by the original authors. 

Indeed, by the standards of clinical trials research, nearly every study we replicated in this report would be classified as exploratory research. No protocol was pre-registered, and the general pattern of evidence across studies was often more important to the authors' conclusions than any particular test in any single study.  Almost every study conducted multiple statistical tests, often associated with several statistical models with differing specifications. And in many cases, there was no conventionally-reported and easily calculable effect size measures [e.g., for mixed- and random-effect models, cf. @bakeman2005]. This set of features -- which, in our experience, are endemic to published psychological work -- makes replication research within the conventional statistical paradigm extremely challenging. 

In our view, two recommendations will improve this situation dramatically. First, pre-registration typically requires the selection of one or a small number of statistical analyses to be designated as critical for interpretation. Were this information available, it would have simplified the problem of selecting effects to replicate in several cases. Second, the widespread adoption of open data practices can dramatically facilitate the type of meta-research pursued here, in a number of ways. In addition to clarifying the (often terse) reporting in a paper, the availability of data facilitates the computation of the relevant meta-analytic variables.^[We note that, although we were grateful that several papers in our sample shared data and materials, the use of open (non-proprietary) formats for archiving data and analytic code is often critical for other investigators attempting to reproduce analyses.] No paper can perfectly predict the uses to which future investigators will put a piece of work; the availability of the source materials is a way of allowing more flexible reuse. 

In conclusion, our results here demonstrate the practical possibility of performing replication research in the classroom. There are many challenges for ensuring high-quality replications in a pedagogical setting -- from checking experimental methods to reviewing analytic code for errors -- but we would argue that these are not just pedagogical challenges. They are challenges for psychological science. We believe that the openness and transparency we pursued here as part of our pedagogical goals should be models not just for other classes but for future research more broadly. 

# Appendix: Project-By-Project Methods Addendum


```{r, fig.height=9, fig.width=6, fig.cap="Side-by-side plots for each attempted replication. Error bars show standard error of the mean. Original data estimated from figures when not otherwise available. Replications are sorted and color coded by subjective replication success."}
  # colors:268bd2 859900 dc322f
library(png)
layout(matrix(1:24,nr=6,byr=T))
project_key_sorted <- c(
  "jsalloum","ehermann","salehi", # rep = 1
  "mkeil","smraposo","jedtan","rhiac","lampinen", # rep=.5
  "jreynolds","jmarias","auc" #rep = 0
)
for (key in project_key_sorted) {
  orig <- readPNG(paste0("../figures/",key,"-original.png"))
  rep <- readPNG(paste0("../figures/",key,"-replication.png"))

  par(mai=c(0,0,0,0))
  plot(c(0, 100), c(0, 100), type = "n", xlab="", ylab="", axes=FALSE)
  rasterImage(orig, 0, 0, 100, 100, interpolate=FALSE)

  plot(c(0, 100), c(0, 100), type = "n", xlab="", ylab="", axes=FALSE)
  rasterImage(rep, 0, 0, 100, 100, interpolate=FALSE)

}
# Add legend
legend <- readPNG(paste0("../figures/Replication-Legend.png"))

plot(c(0, 100), c(0, 100), type = "n", xlab="", ylab="", axes=F)
  rasterImage(legend, 0, 0, 100, 100, interpolate=FALSE)
```

For each project, we present a summary of the finding, the key statistical test, and the impressionistic outcomes of the replication. Figure 3 shows a side-by-side comparison of the key visualization for each study.

## Storm & Stone (2015)

This paper found that when participants had the opportunity to save a file containing a list of target words before studying another file, they retained more information from the second study session. Our target for replication was Experiment 3, in which the effect was found to depend on the amount of information that was studied in the first file, with a greater effect for an eight-word list than a two-word list. The key statistical test we selected was the interaction in a 2x2 mixed-design ANOVA (save vs. no-save condition and two- vs. eight-word condition). The replication study failed to find evidence of the interaction, but ironically this was because we found evidence for a main effect of save condition: The authors' key manipulation succeeded for both load conditions. Perhaps even two words was sufficient to create interference for our online sample.

## Lewis & Oyserman (2015)

This paper investigated motivations to save for retirement, and specifically whether changing participants' relationships to the future (making it feel more imminent) would lead them to report that they would begin saving for retirement sooner. Our target was Study 4, which found that seeing the time to retirement in days (10,950) rather than years (30) caused participants to say they would begin saving sooner, even controlling for age, education, and income in a linear model (there was also a manipulation of whether saving was incremental that did not result in an effect). We failed to find a significant relationship between the time metric manipulation and saving time. However, when we did not include demographic controls, we did find a marginally-significant relationship. In exploratory analyses, we also observed an un-predicted effect of income such that participants who reported higher income selected to start saving sooner.

## Scopelliti, Lowenstein & Vosgerau (2015)

The goal of this paper was to examine whether people are accurately calibrated in their estimates of how others will respond to their attempts at self-promotion. Our target was Experiment 3, which showed that participants, who were instructed to describe themselves in such a way that others would like them, expected to be liked more -- but were actually liked less -- than participants who were asked to describe themselves but not additionally instructed to maximize others’ interest in meeting them. The authors performed a series of regressions on four dependent variables capturing judgments about profile writers (interest, liking, bragging, and success), analyzing these as a function of condition (control vs. "maximize interest") and who the evaluator was (the writer vs. an independent sample). Power analysis for this design was quite complex because the procedure had two stages: in a first stage participants generated descriptions and predicted judgments, while in a second stage a separate sample rated the descriptions. 

We were faced with a choice: either we could have re-rated the descriptions gathered in the original study (by contacting the author and treating these as "materials" for the study), effectively replicating only stage two, or we could redo the entire two-step procedure. We elected to conduct a replication of the full procedure, but in our replication to decrease cost we focused on the liking judgments, which appeared to be central to the authors' conclusions. Our key test was the interaction of rater (writer vs. independent) and condition.^[We note that this project was submitted late and the student failed to preregister this analysis formally; however, the pre-written analysis protocol went through pre-data collection review by MCF.]  However, to power even this smaller study adequately was outside of our budget, so instead of having each judge rate a subset of 10 out of 100 total profiles, we collected a smaller sample of profiles (18) and had every profile rated by each judge. We failed to find the predicted interaction in this new sample. Instead, we found a main effect such that stage one participants predicted that raters would like them more than the stage two participants actually did. Despite our relatively high statistical power in the main test, however, we may have suffered from low power at the item level due to the relatively small number of profiles we collected in the replication study.

## Liverence & Scholl (2015)

This paper tested the theory that persistent object representations could assist in spatial navigation, using a navigation paradigm in which participants used key presses to move though a grid of pictures that were visible only one at a time. In Experiment 1, our target, found that participants located targets faster when navigation involved persistence cues (via sliding animations) than when persistence was disrupted (via temporally matched fading animations). The key test for our replication was a simple *t*-test comparing speed to find the target in the two conditions (slide vs. fade), not controlling for learning across the duration of the experiment. We failed to find this overall difference. However, in an exploratory followup analysis using a linear mixed effect model, we did find a highly-significant effect of condition when controlling for learning epoch. We speculate that the very large effect found by the original authors may have led them to select a simpler analysis that did not control for learning effects. Our data -- which were more variable -- required this control.

## Wang et al. (2015)

This paper reported that the relationship between math anxiety and math performance follows an inverted-U pattern among college undergraduate students with higher intrinsic math motivation, whereas this same relationship is linearly negative among students with lower intrinsic math motivation. We targeted Study 2, the authors' own replication study, in which participants filled out three surveys about their math motivation and anxiety and completed a math task to measure performance. The key effect we targeted was the interaction of math anxiety squared and math motivation in predicting performance. We failed to find evidence for this effect. In contrast to the original study, we found a simple linear trend such that performance decreased with increasing math anxiety for both high and low math-motivation groups. 

## Sofer et al. (2015)

This paper tested the hypothesis that face typicality affects trustworthiness judgments such that the most typical face will be judged to be the most trustworthy. In Study 1, our target, faces of varying typicality were rated on their trustworthiness and attractiveness; while trustworthiness peaked at the most typical face, less typical faces were judged more attractive. While the original class projects selected a global, item-wise regression model as the key statistical test of interest, the instructor team (post-data collection) determined that the fit of this model did not in fact correspond most closely with the authors' stated hypothesis. We thus selected the interaction between face typicality and rating condition as our key test statistic and used the more standard by-participant regression model (which the original paper also reported subsequent to the by-items model) as the target model. Our replication study successfully showed this same interaction.^[We were unable to reproduce the numerical results from this model using the data from the original study, but we report the (larger) test statistics from our recomputed version.] We note that, despite the significant interaction, in our study trustworthiness judgments did not change substantially with typicality (though attractiveness did) and trustworthiness did not peak numerically at the most typical face. Thus, despite replicating one key statistical test, our results are at odds with some of the original paper's claims. 

## Ko, Sadler, & Galinksy (2015)

This paper investigated vocal cues to social hierarchy. Our target for replication was Experiment 2, which assessed whether participants used particular acoustic cues to make inferences about speakers' level of social hierarchy, particularly that participants from the high-rank condition in Experiment 1 were rated as more highly ranking. This paper employed a two-stage procedure in which participant-generated materials were rated by an independent sample. However, in contrast to the study of Scopelliti, Lowenstein & Vosgerau (2015), because materials were collected in Experiment 1 and were available openly, we elected to replicate only Experiment 2, the judgment study. The key test statistic was the main effect of hierarchy condition on "behavior score" (an index of whether speakers would engage in high-status behaviors). The authors were also interested in which particular vocal cues predicted participants judgments, but we judged these analyses to be more descriptive and exploratory. We successfully replicated the main effect of condition on hierarchy judgment, and additionally found a comparable main effect of speaker gender. 

## Atir, Rosenzweig, & Dunning (2015)

This paper investigated how perceived knowledge affects "overclaiming" (claiming knowledge that you do not have). We targeted Study 1b, which asked participants to rate their knowledge about finances, and then asked them to rate their knowledge about a set of financial terms, some of which were invented (and hence for which a positive knowledge statement would necessarily be an overclaim). The original study found that participants with higher self-rated financial knowledge also overclaimed at a higher rate. The key test statistic for this study was the coefficient in a regression model predicting overclaiming as a function of claimed personal financial knowledge, controlling for accuracy in the true financial knowledge questions. We successfully replicated this effect, despite our use of a much smaller sample (due to the extremely high post-hoc power of the original study). 

## Proudfoot, Kay, & Koval (2015)

This paper examined whether judgments of creativity are influenced by the gender of the creator. In Study 1, they tested the hypothesis that "out of the box" creativity was more associated with masculine traits than feminine traits by manipulating the definition of creativity that participants saw (convergent vs. divergent) and asking participants to judge the centrality of different traits to that definition. The original pre-registration selected a subsidiary analysis of a main effect as the key test statistic, but in post-data collection review the instructor team decided that a test of the interaction between trait gender and definition condition was closer to the central theoretical claim of the paper; we use this latter test as the key test statistic. Our replication study found an significant interaction of similar magnitude.
 
## Zaval, Markowitz, & Weber (2015)

This paper investigated whether encouraging participants to think about their personal legacy could increase concern for the environment. The main study of the paper investigated whether a legacy priming writing exercise would increase a variety of environmental measures, including donations to an environmental charity, pro-environmental intentions, and climate change beliefs. The key test we selected was the effect of condition (priming vs. control) on behavioral intentions in an analysis of variance. We observed a marginally significant effect in the same direction in our replication, as well as support for the mediating relationship of legacy motives on behavioral intentions. Our study may have been under-powered due to budgetary constraints; we ran a sample of comparable size to the original sample of 312 participants. We also may have observed a smaller effect size due to the relatively smaller amount of time our participants spent on the legacy prime writing exercise. 

## Xu & Franconeri (2015)

This paper explored constraints on visual working memory in the context of mental rotation. In Experiment 1a, participants performed a verbal suppression task to control for use of verbal encoding while attempting to remember a cross with four colored bars, which sometimes switched colors; performing mental rotation significantly impaired ability to detect these bar swaps. To port the task to an online framework we decreased the number of trials and increased the number of participants we tested. The key test statistic was a comparison of K (a metric of the number of visual features remembered) between the rotation and no-rotation conditions. We observed a marginally-significant effect in our replication study. Our study showed a floor effect on capacity such that participants remembered on average very little; it is possible that small details in the displays (or the differing sample available online) led to this floor effect. 

# References 





<!-- # Supplemental -->

<!-- All materials that are needed for class to implement -->
<!-- Email templates -->
<!-- Spreadsheets -->
<!-- TA/prof auditing processes -->