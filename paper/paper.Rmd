---
title: "Improving the Replicability of Psychological Science Through Pedagogy"
short-title: "Replication Through Pedagogy"
output: kmr::apa_manuscript

header-includes:
   - \usepackage{pdflscape}
   - \usepackage{multicol}
   
csl: apa6.csl
bibliography: replication254.bib

document-params: "a4paper,man,apacite,floatsintext,longtable"

bib-tex: "replication254.bib"

author-information:
    - \author{Robert X. D. Hawkins*, Eric N. Smith*, Carolyn Au, Juan Miguel Arias, Rhia Catapano, Eric Hermann, Martin Keil, Andrew Lampinen, Sarah Raposo, Jesse Reynolds, Shima Salehi, Justin Salloum, Jed Tan, and Michael C. Frank}

affiliation-information:
    - \affiliation{Department of Psychology, Stanford University}

author-note:
    "*These authors contributed equally and are listed alphabetically."
    
abstract: 
    "Replications are important to science, but who will do them? One proposal is that students can conduct replications as part of their training. As a proof-of-concept for this idea, here we report a series of 11 pre-registered replications of findings from the 2015 volume of Psychological Science, all conducted as part of a graduate-level course. Congruent with larger, more systematic efforts, replications typically yielded smaller effects than originals: The modal outcome was partial support for the original claim. This work documents the challenges facing motivated students as they attempt to replicate previously published results on a first attempt. We describe the workflow and pedagogical methods that were used in the class and discuss implications both for the adoption of this pedagogical model and for replication research more broadly."
    
keywords:
    "Replication; Reproducibility; Pedagogy; Experimental Methods"
---

```{r global_options, include=FALSE}
rm(list=ls())
library(knitr)
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = FALSE, 
                      fig.path='figs/', echo=FALSE, warning = FALSE, 
                      cache=TRUE, message = FALSE, sanitize = TRUE)
#devtools::install_github("kemacdonald/kmr")
library(readr)
library(dplyr)
library(xtable)
library(readr)
library(tidyr)
library(ggplot2)
library(BayesFactor)
library(ggrepel)
#devtools::install_github("langcog/langcog")
library(langcog)
library(cowplot)
library(png)
```

<!-- The problem, a solution, and the current lack of evidence for it -->
Replicability is a core value for empirical research and there is increasing concern throughout psychology that more independent replication is necessary [@osc2015;@wagenmakers2012]. 
Yet under the current incentive structure for science, replication is not typically valued for publication in top journals [@makel2012] or in metrics of research productivity [@koole2012]. 
One potential solution to this problem is to make replication an explicit part of pedagogy: that is, to teach students about experimental methods by asking them to run replication studies [@frank2012;@grahe2012]. 
Despite enthusiasm for this idea [@lebel2015;@everett2015;@standing2016;@king2016], there is limited data beyond anecdotal reports and individual projects [e.g., @phillips2015;@lakens2013] to support its efficacy in producing wide-scale pedagogical adoption. 

<!-- The kind of evidence we're going to provide -->
In the current article, we describe the pedagogical and methodological approach to replication research taken in our graduate-level experimental methods course and address the practical barriers faced by instructors planning to incorporate replications into their courses.
In our course, students conducted replications of published articles from the 2015 volume of the journal *Psychological Science* with rigorous instructor review at each major stage. 
The results of these replications are a microcosm of larger replication efforts, providing insight into both the difficulties of pedagogical replications and their promise as a method for improving the robustness of psychological research.

<!-- The scientific value of that evidence  -->
We assess the challenges facing a student in choosing an article of interest and -- in a single attempt, within constraints of budget, expertise, and effort -- reproducing the findings. 
We consider a number of criteria for evaluating replication success, including statistical significance, effect size, a Bayesian measure of evidence [@etz2016], and a subjective assessment with respect to the original authors' interpretations. 
While each of these is imperfect, taken together these measures help provide a sense of the robustness and generalizability of an effect, and perhaps more importantly, how easy it is for a student to reproduce an effect to the degree that they could confidently build on it in their own future work. 
<!-- But, as we discuss below, the results of these replications may have different interpretations: While some are more direct, others differ on theoretically-relevant dimensions. Indeed, due to differences in population, timing, or method, some of our findings might be construed as assessing the boundary conditions of a particular effect rather than replicating the original exactly. Even in these cases, however, replication research of the type we conduct here contributes to an understanding of the robustness, generalizability, and expected effect sizes of previously published findings. -->

<!-- The educational value of that evidence  -->
We also describe in detail our process for conducting replications as part of classroom pedagogy. 
Although mentorship in experimental methods is an important part of the standard advising relationship in psychology, the classroom context allows for the elucidation of general principles of good research and discussion of how they can be modified to fit specific instances. 
And replication research in particular illustrates a number of important concepts -- experimental design, power analysis, reporting standards, and preregistration, among others -- more directly than open-ended projects, which require new conceptual development [see @frank2012 for extended argument]. 

<!-- Inspirational stuff and a call to join us... -->
There are significant limitations on what can be done in a single term, within the constraints of a course budget and the instructors' expertise. 
Nevertheless, were this approach implemented more widely, we believe the dividends paid to the field as a whole -- both scientific and educational -- would be considerable. 
To encourage others to share our materials, our course outline, project templates, and assignments are available publicly at [https://osf.io/98ta4/files/](https://osf.io/98ta4/files/). 
In addition, all of our most recent lecture slides and materials are available on our course website at [http://psych254.stanford.edu](). 
We hope that others will share and reuse our materials as they consider how best to design a course customized to their aims and context. 

<!-- We begin by reporting the empirical results our replications, highlighting the importance of wide-scale replication efforts. We then provide the particular details of our course projects to give a sense of what a classroom replication project entails. We close with a discussion of broader decision points for the course design that other instructors might consider.  -->

```{r, results='asis'}
project_plan_raw <- read.csv("../data/planning_and_outcomes.csv")

project_plan <- project_plan_raw %>%
  filter(`include_in_final_writeup`==1) 

project_data_summary <- read_csv("../data/project_data_summary.csv")

d <- left_join(project_plan, project_data_summary, by = c("project_key")) %>%
  rowwise() %>%
  mutate(fidelity = (replication_fidelity_MCF + replication_fidelity_ES + replication_fidelity_RH) / 3) %>%
  ungroup
```

\begin{landscape}
\centering

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{lllllr|lrcl}
  \hline
  
 & \multicolumn{5}{c|}{\underline{Original Study}} & \multicolumn{4}{c}{\underline{Replication Study}} \\ 
  
\specialcell{Citation\\(Psychological Science; 2015)} & Expt. & \specialcell{Open\\Data?} & \specialcell{Open\\Materials?} & \specialcell{On\\MTurk?} & \specialcell{N\\(orig)} & \specialcell{Power\\Standard} & \specialcell{N\\(rep)} & \specialcell{Instructor\\Fidelity}\\ 
  \hline
Atir, Rosenzweig, \& Dunning & 1b & No & Yes & Yes & 202 & Other &  50 & 6.67 & \\ 
Ko, Sadler, \& Galinksy & 2 & Yes & Some & No &  40 & Original &  40 & 4.67 &\\ 
Lewis \& Oyserman & 4 & No & Yes & Yes & 122 & 80\% power & 128 & 6.67 &\\ 
Liverence \& Scholl & 1 & No & Some & No &  18 & Original* &  19 & 5.33 &\\ 
Proudfoot, Kay, \& Koval & 1 & No & No & Yes &  80 & 80\% power &  84 & 6.67&\\ 
Scopelliti, Loewenstein, \& Vosgerau & 3 & Yes & Yes & Yes & 550 & Other & 124 & 5.67& \\ 
Sofer et al. & 1 & Yes & Yes & No & 48 & Other &  95 & 4.33& \\ 
Storm \& Stone & 3 & No & No & No & 48 & Original &  61 & 4.00& \\ 
Wang et al. & 2 & No & No & No & 219 & 80\% power & 397 & 4.33&\\ 
Xu \& Franconeri & 1a & No & No & No &  12 & Other* &  27 & 5.00& \\ 
Zaval, Markowitz, \& Weber & 1 & Yes & Yes & Yes & 312 & Original & 321 & 5.00&\\ 
   \hline
\end{tabular}
\caption{\label{tab:summary} Summary characteristics of original studies and our replications. All project materials available publicly at osf.io/98ta4/files/, and links to individual project preregistrations, reports, and web experiments are available at osf.io/98ta4/wiki/. * marks projects where the number of trials was modified.} 
\label{tab:summary}
\end{table}

\end{landscape}

# Disclosures

## Preregistration

<!-- Preregistration: Provide a link to any preregistration documentation. If only a subset of reported studies were preregistered, indicate which ones were and which ones were not. -->

As described below, our procedure for individual project preregistrations was to register the analytic script with specific key hypothesis tests clearly marked. 
Individual project preregistration links are given at [https://osf.io/98ta4/wiki/](osf.io/98ta4/wiki/). 
Prior to data collection for all projects, we also pre-registered the confirmatory analyses reported in the current paper at [https://osf.io/rxz8m/](https://osf.io/rxz8m/). 

## Data, Materials, and Online Resources

<!-- Data, materials, and online resources: Provide a permanent, persisting link to a public archive where readers can access any code, materials, de-identified data, and other resources (e.g., osf.io, perma.cc, clinicaltrials.gov). If any materials or data are not publicly available, the article should explain why and should note how researchers can access them for research purposes. -->

All code and data necessary to reproduce the analyses reported here are available at [https://osf.io/98ta4](https://osf.io/98ta4).

## Measures

<!-- Measures: The text here should state “We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study” (see Simmons, Nelson, & Simonsohn, 2011). If any aspect of this statement is untrue, authors should explain why. All such details should be specified in the preregistration documentation as well. -->

We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study (see @simmons2011).

## Subjects

<!-- Subjects: If the article reports research with human or animal subjects, authors should state that testing was conducted with the approval of an institutional review board or similar ethics committee (with the protocol number provided). For those involving human subjects, authors should state whether the research was conducted in accordance with the Declaration of Helsinki; note that the latest version of the Declaration of Helsinki requires preregistration before data collection begins. -->

All replications were approved by the Stanford University Institutional Review Board under protocol \#23274, "Reproducibility of psychological science and instruction" and were conducted in accordance with the Declaration of Helsinki.

## Conflicts of Interest

<!-- Conflicts of Interest: Authors should identify any conflicts of interest here and should also report them during the submission process. If authors have no conflicts of interest, they should state, “” -->

The authors declare that they have no conflicts of interest with respect to the authorship or the publication of this article.

## Author Contributions

<!-- Author Contributions: Authorship implies significant participation in the research reported or in writing the manuscript, including participation in the design and/or interpretation of reported experiments or results, participation in the acquisition and/or analysis of data, and participation in drafting and/or revising the manuscript. All authors must agree to the order in which the authors are listed and must have read the final manuscript and approved its submission. They must also agree to take responsibility for the work in the event that its integrity or veracity is questioned. Authors should precisely describe their contributions to the research and manuscript, identifying each author by his or her initials. For example, “DJS and AOH jointly generated the idea for the study. AOH programmed the study and collected the data. DJS wrote the analysis code and analyzed the data, and AOH verified the accuracy of those analyses. DJS wrote the first draft of the manuscript, and both authors critically edited it. Both authors approved the final submitted version of the manuscript.” -->

RXDH, ENS, and MCF designed the project, supported data planning and data collection for all projects, analyzed the data, and wrote the paper. MCF designed the course.
CA, JMA, RC, EH, MK, AL, SR, JR, SS, JS, and JT planned individual projects, programmed studies, collected data, analyzed data, and gave feedback on the paper.

## Acknowledgements

<!-- Acknowledgments: Authors should use this section to identify any people who should be credited for their assistance with the reported research. -->

Thanks to the Stanford Department of Psychology and the Vice Provost for Graduate Education for funding to support the class. 
We are grateful to the authors of the original studies who provided materials and gave extensive comments on an earlier draft of this manuscript.

## Prior Versions

<!-- Prior versions: If part or all of a submitted manuscript was previously posted to a blog or to a preprint archive, provide a link to those sources and briefly indicate what aspects of the manuscript are shared between the submitted version and earlier versions. -->

An earlier draft of this manuscript was posted at [https://osf.io/preprints/psyarxiv/p73he/](https://osf.io/preprints/psyarxiv/p73he/).

# Methods

All projects were completed as part of a graduate-level methodology class. 
At the initiation of class, all students were told that they had the opportunity to contribute their individual class assignment to a group replication project. 
The requirements for joining the project were to conduct a pre-registered replication of a finding from the 2015 volume of *Psychological Science* and to contribute the code, data, and materials to the writeup. 
A schematic of our class timeline is shown in Fig. 1.

```{r, fig.cap="A schematic view of our class timeline for replication projects. R indicates the approximate timing of instructor team reviews of student materials.", fig.height=4, fig.width=6, fig.align="center", fig.pos="t"}

img <- readPNG(source = "../figures/pipeline.png")
plot.new()
plot.window(0:1, 0:1)

#fill plot with image
usr<-par("usr")    
rasterImage(img, usr[1], usr[3], usr[2], usr[4])
```

A summary of the original empirical studies chosen for replication is given in Table \ref{tab:summary}. 
Students chose findings from a wide variety of domains for replication. 
One cluster of studies included investigations of memory and visual attention [e.g., @xu2015], for example in applied contexts like remembering "deleted" files [@storm2015] or tracking transitions between locations on smartphone apps [@liverence2015]. 
A second cluster came from studies of social perception and social judgment, including judgments of faces and voices [@ko2015;@sofer2015], as well as studies of attributions of modesty, creativity, and expertise [@atir2015;@scopelliti2015;@proudfoot2015]. 
Other studies investigated increasing retirement savings by orienting participants to the future [@lewis2015], legacy priming [@zaval2015], and the effects of math anxiety on performance [@wang2015]. 
Details of individual replications are available in Supplemental Material. 

## Participants

All study participants were recruited on Amazon Mechanical Turk (AMT). 
Individual sample sizes are given in Table \ref{tab:summary}.
Each sample was recruited independently, using the same AMT account but a different task title.
Of the `r nrow(d)` studies included in the final sample, `r sum(project_plan$original_on_turk=="Yes")` (`r 100 * round(mean(project_plan$original_on_turk=="Yes"), digits=2)`%) were originally conducted on AMT. 
For the other studies in our sample, demographic differences between the original sample and the AMT sample (in terms of age, sex, socio-economic status, and in some cases, national origin) are a factor in interpreting our findings; see Supplemental Material for more discussion of this issue in individual cases.^[Repeat administration of empirical work on AMT has been raised as an issue in some prior work [e.g., @chandler2014], but tracking participation in specific paradigms is an open challenge. We did not ask participants whether they had participated in similar research previously, as we suspected that asking this sort of question would lead to a large number of inaccurate responses due to failures to distinguish between related experimental paradigms. In addition, half of the AMT population is estimated to change every six months [@stewart2015], and we expected that most of the studies in our sample that had been conducted on AMT had been performed at least a year previously (though see Supplemental Material for discussion of one particular paradigm that has been used more recently).]

We determined sample sizes using a case-by-case selection criterion that attempted to maximize the success of the project while staying within the constraints of our budget (see below for discussion).^[We initially were allocated \$1,500 but one student contributed personal research funds, leading to a total cost of $\approx$ \$1,700 for all studies. We attempted to set payment for our studies at approximately \$6/hour based on timing estimated from Pilot A sessions [@salehi2015].] 
To ensure they still achieved the desired criterion if some participants skipped through or did not complete the study, experimenters recruited 5% more participants than specified by their criterion. 
In four cases, we powered the replication attempt to 80\% power based on post-hoc power calculations of the original effect sizes. 
In four cases, we used the original sample size. 
In one case, the original study was powered well above what would be required to find the effect of interest based on the post-hoc analysis, so we chose a smaller sample size that yielded sufficiently high power to detect that effect. 
In two others, we reduced the number of trials as part of the adaptation to Mechanical Turk (in one of these, changing the sample size to compensate). 
In the last case, the original study used a multi-step procedure where the first step consisted of stimulus generation; we decreased the N in the first stage of the procedure and focused on achieving high power at the second stage.  

## Materials and Methods

Students used JavaScript and HTML/CSS to reimplement their respective target studies in the web browser, otherwise following the methods specified by the original authors as closely as possible.
Of the `r nrow(d)` studies included in the final sample, `r 100 * round(mean(project_plan$open_materials == "Yes" | project_plan$open_materials == "Some"), digits=2)`% of studies had either some or all materials openly available. 
For `r sum(project_plan$author_contact != "")` studies, students contacted the authors to request either materials or clarification of methods using a template email that was customized by the students and reviewed by the instructors (this template is included in our course materials). 
Responses were received in all but one case, all within a matter of days.  

## Milestones, Preregistration, and Review

In addition to as-needed guidance on projects, students advanced through a formal review process in which each student's work was inspected and critiqued several times by both the instructor and the teaching assistants (see Fig. 1). 
In the second week of the course, students wrote a brief proposal selecting their replication targets, which were reviewed for feasibility and concreteness. 
After their selection was approved, students proceeded to write the methods section of the @osc2015 replication report template. 
This section included a power analysis, proposed sample size, and explicit description of differences from the original study (see course materials for complete template). 
At the same time, students worked on an experiment "mock-up": a fully functional but unpolished outline of their experiment, potentially using placeholders for one or more unfinished elements of the design. 
Upon reviewing the completed methods and mockup, the teaching staff gave thorough feedback and suggested changes to be made before beginning any data collection.

For each project, further reviews were triggered by the completion of two pilot samples. 
The first, "Pilot A," consisted of a handful of non-naive participants (e.g., the experimenter, other students). 
The goal of this pilot was to ensure that all needed data were being accurately logged and that analytic scripts for the confirmatory analyses functioned appropriately. 
After Pilot A was completed, the instructor or a TA critiqued and reviewed the student's experimental script, analytic code, and resulting output. 

Once requested changes were made, the student conducted "Pilot B," using a handful of naive participants recruited from AMT. 
The goal of this pilot was to ensure that participants were able to complete the study and did not report any substantial issues with instructions or technical details of the study (all students were instructed to give participants a way to leave free-form comments at the end of the study, prior to debriefing). 
At the conclusion of Pilot B, *both* the instructor and a TA reviewed the student's analytic code and its outputs on the data for all confirmatory analyses. 
The goal of this code review was to ensure that all planned analyses were specified for the key test selected in the original article, and that all relevant exclusion criteria, manipulation checks, etc. specified by the original authors were implemented correctly.

After Pilot B review was completed, students were given authorization to collect the full sample of participants, contingent on having made any requested changes.
Prior to data collection, the analytic script containing all confirmatory analyses was pre-registered using the Open Science Framework.

<!-- Using the template developed by @osc2015 for replication reports (included in our course materials), students first specified the scope and methods of their proposed replication study, including any differences from the original methods. Later, they added the code necessary to analyze their data, and after collecting their full sample, the results of their data analysis (e.g., statistical tests and figures).  -->


## Statistical Approach

Despite the general emphasis in meta-analytic and reproducibility efforts on aggregating a single effect size of interest [@osc2015;@lipsey2001], it was often challenging to identify a single key statistical test for each study. Usually a substantial number of statistical tests were conducted, often within one or several ANOVA or regression models with multiple specifications. Thus, we interpret "key statistical test" results with caution (see Empirical Results and Discussion below). Despite this interpretive difficulty, we followed previous work and attempted to use a single statistical test as our target for replication planning and analysis.

# Pedagogy: Results and Discussion

## Results: Pedagogical Assessment 

All of the 15 students in the class completed projects (two with extensions going beyond the class period). Of these, one chose a project outside of the sampling frame, and three opted out of the broader project prior to data collection.^[These students did not express doubts about the success of their projects prior to opting out, and 2/3 of these projects were judged by the instructor team to have been successful replications. Thus we have no evidence that these students opted out systematically due to their belief that their project would fail.] The remaining 11 students contributed code, data, registrations, and materials to the final product. (In one case, a student did not complete the pre-registration procedure correctly but still submitted a pre-specified analysis plan to the instructors for review. We included this project in the final analysis.) 

Because they were conducted by students within the constraints of a course, our studies had a number of limitations, including: 1) limited time for iterative piloting and adjustment, 2) limits on funding for larger samples, and 3) limits on domain expertise with respect to the specific effects that were chosen. But within these limitations, our group was able to produce a set of relatively close replications with generally good statistical power. Thus, our work here shows what is possible for motivated students using freely-available tools and resources.

## Discussion: Pedagogical Implications

Our findings demonstrate that classroom replication projects are possible in one particular context. In the following subsections, we provide information that we hope will reduce barriers to entry and encourage more researchers and instructor to include replications as part of coursework in their courses. We discuss practical recommendations for implementation in a variety of classroom settings, key decisions that instructors must make, and common issues that arise during replication projects. 

### Models for students of different levels

Though others' courses that incorporate replication projects will look quite different from our own, we recommend having the replication as a centerpiece of the course and giving ample opportunity for feedback throughout the study design. Without a central focus on the replication project, students will not have the time or motivation to complete a high quality project; without multiple opportunities for extensive (and often interactive) instructor feedback, the quality of the final projects will decline. 

That said, depending on the student population and class constraints, there are many ways to include a replication project in a course. Table \ref{tab:classmodels} presents three different possible models of replication projects, for general undergraduate classes, advanced undergraduate/early graduate classes, and more advanced graduate classes. These models are not meant to be binding suggestions, but we hope that they inspire instructors to consider how replication research could be included in their own teaching practice. The sections below provide some discussion of individual choice-points. 

\begin{center}
\begin{landscape}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\begin{table}[ht]
\footnotesize
\centering

\def\arraystretch{1.2}

\begin{tabular}{lccc}

 & \multicolumn{3}{c}{\underline{Classroom Models}} \\
  
\multicolumn{1}{p{1.5in}}{\raggedright \underline{Course Attributes}} & \underline{Model 1} & \underline{Model 2} & \underline{Model 3} \\

Student group & \multicolumn{1}{p{2in}}{\raggedright Mid-level undergraduates} & \multicolumn{1}{p{2in}}{\raggedright Advanced undergraduates or graduate students} & \multicolumn{1}{p{2in}}{\raggedright Advanced graduate students} \\

\hline

Learning goal & \multicolumn{1}{p{2in}}{\raggedright Gain experience with research process} & \multicolumn{1}{p{2in}}{\raggedright Gain research independence} & \multicolumn{1}{p{2in}}{\raggedright Master best practices and tools} \\

Project group size & \multicolumn{1}{p{2in}}{\raggedright Full class or medium-size groups (7-8 students)} & \multicolumn{1}{p{2in}}{\raggedright Small groups (2-4) or individuals} & \multicolumn{1}{p{2in}}{\raggedright Individuals} \\

Project selection & \multicolumn{1}{p{2in}}{\raggedright Full class replication or choose from small curated list} & \multicolumn{1}{p{2in}}{\raggedright Choose from curated list} & \multicolumn{1}{p{2in}}{\raggedright Open project selection or meta-science sampling frame} \\

Subject population & \multicolumn{1}{p{2in}}{\raggedright In-person convenience samples} & \multicolumn{1}{p{2in}}{\raggedright University pool or AMT} & \multicolumn{1}{p{2in}}{\raggedright AMT or targeted sampling of original populations} \\

Workflow tools & \multicolumn{1}{p{2in}}{\raggedright GUI tools, instructors code studies} & \multicolumn{1}{p{2in}}{\raggedright Teach one key coding-based tool, GUI tools otherwise} & \multicolumn{1}{p{2in}}{\raggedright Teach full ecosystem of key coding tools} \\

Pre-registration & \multicolumn{1}{p{2in}}{\raggedright Pre-registration by instructor} & \multicolumn{1}{p{2in}}{\raggedright Pre-registration drafted by students} & \multicolumn{1}{p{2in}}{\raggedright Pre-registration by students with instructor review} \\

Dissemination & \multicolumn{1}{p{2in}}{\raggedright If meeting pre-specified quality standard} & \multicolumn{1}{p{2in}}{\raggedright With instructor approval} & \multicolumn{1}{p{2in}}{\raggedright As class default, subject to review} \\


\end{tabular}
\caption{\label{tab:classmodels} Three potential models for incorporating replication projects into classes of different levels.} 
\label{tab:classmodels}
\end{table}

\end{landscape}
\end{center}
<!-- Review & \multicolumn{1}{p{2in}}{\raggedright Standardized review checkpoints} & \multicolumn{1}{p{2in}}{\raggedright Extensive group review and minimal instructor review} & \multicolumn{1}{p{2in}}{\raggedright Multiple reviews between students and with instructors} \\ -->

### Project selection 

In our class, students selected projects based on interest, and we encouraged students with little programming background to opt for methodologically simple studies, and more advanced students to take on a challenging design. This level of freedom is not ideal for less advanced courses, and the set of available replication studies should likely be curated for students at lower levels. For example, in a large undergraduate class it may be logistically simpler for the instructors to choose a small set of replication projects that students can select from, or perhaps even pool resources and collect multiple datasets for a single project. Limiting the set of projects both gives instructors a greater degree of familiarity with the relevant literature and allows for the development of more scaffolding of experimental materials. A mid-level course might split the difference between these two models, providing students with a broader -- but still curated -- list of potential projects. 
 

<!-- However, this sample also reflects what is of most interest to beginning researchers, and we suspect may be a better sampling frame to better understand particularly impactful effects to be built upon in future research. The modal study chosen was the first, but several students chose later studies in a set as well.  Thus, all interpretation of our findings should be with respect to this non-probability sample.  This sampling strategy may thus bias our results towards more exciting or novel findings, as well as those that were more feasible for students. -->

### Subject population and sample size

For pedagogical replications to also make a *scientific* contribution, they must be adequately powered and use subject populations comparable to those used in typical lab research. With these needs in mind, we chose to use Amazon Mechanical Turk (AMT) for our replication studies. AMT facilitates the process of recruiting diverse, high-quality samples [@BuhrmesterKwangGosling11;@CrumpEtAl13_Turk] that are large enough to enable replications of between-subjects designs (which typically require large samples). Because psychological research is increasingly conducted online, an additional pedagogical benefit is to build competence in these skills. Even if students’ current research does not use AMT, these replications allow students to build a skillset that can be used throughout their academic careers. 

Still, collecting sufficiently large samples through AMT may strain financial resources. Depending on the number of target studies, the needs of those studies, the level of the student group, and the total funds allocated to the course (e.g., through teaching grants or departmental support), other populations may be more feasible and appropriate. For instance, recruitment from community samples or university participant pools may provide high-quality data while reducing costs and eliminating a technical challenge in data collection.

Cost considerations also enter decisions about determining appropriate sample sizes, a major challenge in replication research [@button2013;@simonsohn2015]. Sample planning based on analysis of the achieved effect size in a previously-published study is problematic because of the likelihood of inflation of published effect sizes due to the "winner's curse" [@hoenig2001;@button2013]. But it may not be feasible to follow more conservative guidelines in all cases. For example, @simonsohn2015 recommends 2.5x the original sample, which can be feasible for small or under-powered original studies, but be unnecessary for studies that were initially large and/or adequately powered. 

As a rule of thumb, for the type of replication research advocated here, conducting a small number of high-powered studies using representative samples is preferable to conducting a large number of under-powered studies in convenience samples: Results are much more likely to be to be interpretable by students and to make a contribution to the replication literature. Regardless of the eventual sample collected, we find that the discussion of subject population & sample size determination is one of the most eye-opening parts of our class and so we encourage instructors to allow students to participate in this planning process.


<!-- as well as to help students learn about a valuable resource for recruitment whose use requires some specialized knowledge. -->
### Workflow tools

All our studies were coded in JavaScript, HTML, and CSS so as to be run in a standard web browser. All analyses were written using R, a free and open-source platform for statistical analysis.  Students wrote their final reports using [R Markdown](http://rmarkdown.rstudio.com/), a "literate programming" environment in which statistical analysis code can be interspersed with text, figures, and tables. This part of our workflow was extremely useful both pedagogically and for encouraging reproducible research practices: each milestone in our course, corresponding to each phase of a research project, required students to fill in additional sections of a single unified document that would become their final report. This method of writing allows students to easily share their cumulative work to date with instructors via a hyperlink, facilitating review of exposition, results, and code together in one location^[The current manuscript is written in this fashion as well; this writing method is also likely to reduce the frequency of statistical reporting errors [@nuijten2015], given that errors are often introduced by transferring results between statistics and word-processing software packages.]. 

Some of these tools can feel inaccessible or intimidating to students with a more limited programming background. Indeed, our students came from a variety of disciplinary backgrounds; many had relatively little experience with web-based empirical studies or literate, reproducible data analysis. Yet by the end of the course, all gained sufficient proficiency with the suite of technical and conceptual tools necessary to carry out an independent project with support from their peers and the course staff. Our experience suggests that it is possible to convey key concepts in sufficient depth that students can learn to use a broad range of open and reproducible tools for their projects, even if mastery will require further experience. 

Our explicit class goal was to provide students with experiences navigating the full ecosystem of open-source scientific tools. For classes at different levels, however, instructors will likely have different goals. It may be preferable for an intermediate course to focus on a single programming tool (e.g., R) and to use other GUI-based software for creating experiments, so as to minimize the learning burden for students. And for more introductory research methods course, instructors must gauge whether the added difficulty of a programming component is appropriate for the skills and backgrounds of their students.

<!-- In our replications, we opted for a case-by-case weighing of the costs and benefits of increasing power across all studies. We first prioritized achieving the participants required for either 80% or original N (whichever was greater) for the maximum number of studies. This rule benefitted studies which were under the budget allocated for each individual project (i.e., studies that had large effect sizes and short participation times.)  Of those that achieved 80% power with sample sizes that were smaller than the original sample, we reduced the most expensive studies down to N required for 80% power. This allowed us to reach our budget goals with the exception of one particularly expensive study.  Fortunately, this student found external funding to reach 80% power. If this were not possible, we would have likely excluded this study from the pre-registration. -->

### Author contact

Over half of our students contacted original authors to request study materials or clarification on methods and/or analysis, but for the purposes of our class, we chose not to require contact with all original authors. We strongly believe methods and materials should be open to allow for replication and productive science [@nosek2015], and hoped to empower students to engage with and build on the published literature directly -- without personal relationships or even personal contact. That is, we chose to do good-faith replications given all information in the original articles and supplemental online materials. Authors were contacted only in cases where there were issues about access to materials or ambiguities in design or analysis. There are benefits of contacting original authors, however. In addition to professional courtesy, authors can provide valuable feedback or guidance (albeit at the cost of some imposition) prior to data collection, potentially heading off post-hoc debates about methodological choices. In the end, it is up to individual instructors to decide what they believe is best for their students in this area. 


<!-- First, contact is a professional courtesy that lets the authors know that their study is being replicated and in some cases allows the original authors to pre-register their hypotheses or concerns with the replication study. Second, original authors can provide methodological feedback prior to replication. This second is a trickier issue on many fronts.  By requesting feedback, discussion with authors perpetuates the dissemination of insider information or lab-specific folklore on methodology that produces the greatest effects. When these assumptions or limitations of a theory or paradigm are not made explicit in publically-available format, this does not count as a record of science. In fact, some replications have prompted these assumptions to be made explicit by the original authors (CITE?).  In addition, when original materials of a study are not provided, it is on the reader to assume all neccessary information is provided to replicate, and is not due to idiosyncracies. Thus, we would generally advice against contacting authors for feedback or materials expect when replication is not viable without them. -->

### Ethical approvals

Ethical review standards vary across instutitions and countries, but we suspect certain ethical concerns will commonly arise when attempting to implement class replication projects. First, instructors may be concerned that ethics approval is not possible within the time constraints of the class. To mitigate this issue, we suggest contacting ethics bodies well in advance of classes to determine whether to create a standard protocol that encompasses all projects (as we did), or to set a timeline for students and instructors to submit individual study protocols very early in the course. Protocol development for both types -- umbrella and individual -- will be simplified if the study or pool of studies to be replicated is pre-selected by instructors. Second, review boards may be concerned about risks particular to the collection of data by students, e.g., lack of debriefing or greater risk of breach of confidentiality due to novice experimenters. We recommend building training around ethical issues into the syllabus of replication-based classes. This practice is both positive for students and also mitigates potential ethics approval concerns. 

# Empirical Findings: Results and Discussion

## Results

All reported results are from pre-registered, confirmatory analyses. In two cases, in final review of the projects (after data analysis), the instructor team decided that the student's choice of key statistical test was not in fact a strong test of the original authors' primary theoretical claim. In both of these cases, the original authors had fit regression models to the data and there was not a single obvious test that corresponded directly to the authors' hypothesis. After discussion, the instructor team converged on a statistical test that they thought better corresponded to the original authors' intended hypothesis of interest. We report results from these corrected tests here. We return to this issue below, as we believe that test selection is a critical theoretical issue in replication research (see e.g., @monin2016 for discussion).

<!-- **rxdh: re: Dan Simons' comment about the results being cumbersome w/o Table 1 showing individual replication outcomes, maybe we should explicitly state our motivation behind presenting these results and say something along the lines of "to avoid targeting particular authors for potential criticism on the basis of a single replication, we only present summary evaluations aggregated across our replications"** MCF: let's say this in the coverletter? -->

<!-- **rxdh: this next paragraph seems a bit minor and might be better in a footnote?** -->


### Subjective fidelity of replications

Each member of the instructor team coded the fidelity of replications, weighing whether the materials, sample, and task parameters differed from the original studies. We coded projects independently on a scale of 1 -- 7 (with 1 being a loose replication with substantive deviations from the original, and 7 being essentially identical), and then discussed our ratings and made adjustments (without coming to full consensus). The mean rating was `r round(mean(d$fidelity), digits = 2)` (range: `r round(min(d$fidelity), digits = 2)` -- `r round(max(d$fidelity), digits = 2)`). Several studies were essentially identical to the originals, but there was a group of others that included differences in population, number of trials, or method -- all factors that might plausibly have had an effect on the results. 

### Subjective success of replications

After data analysis, for each replication, the student carrying out each study and each instructor gave independent "replication ratings" to assess the subjective success of the replication. Our three-point rating system was based on the theoretical support for the original finding, "None" (no support; 0\%), "Partial" (some support; 50\%), and "Full" (replication consistent with original interpretation; 100\%). Student and instructor ratings were in agreement for `r sum(d$replicated_TA == d$replicated_student,na.rm=T)` out of the `r nrow(d)` replications.  The average rating given by instructors and students was `r 100 * round(mean(d$replicated_TA, na.rm=TRUE), digits = 2)`\% and `r 100 * round(mean(d$replicated_student, na.rm=TRUE), digits = 2)`\% respectively.  

```{r}
d$high_fidelity <- d$fidelity > median(d$fidelity)

replicated <- d %>% 
  group_by(replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  mutate(measure = "replicated",
         value = "")

turk <- d %>% 
  group_by(original_on_turk, replicated_TA) %>%
  summarise(n = n()) %>%
  spread(replicated_TA, n) %>%
  rename(value = original_on_turk) %>%
  mutate(measure = "original on turk") 

fidelity <- d %>% 
  group_by(high_fidelity,replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  mutate(measure = "high fidelity") %>%
  rename(value = high_fidelity) %>%
  ungroup() %>%
  mutate(value = c("Yes","No")[as.numeric(value) + 1])

open_materials <- d %>% 
  group_by(open_materials,replicated_TA) %>%
  summarise(n= n()) %>%
  spread(replicated_TA, n) %>%
  mutate(measure = "open materials") %>%
  rename(value = open_materials)

outcomes <- bind_rows(replicated, turk, fidelity, open_materials) 
names(outcomes) <- c("No","Partial","Full", "Mediator","Value")
outcomes <- outcomes %>%
  select(Mediator, Value, No, Partial, Full)
```

The modal replication project in our sample was judged to be a "partial" replication, meaning that some aspects of the observed findings were different from those reported by the original authors. Since our sample only includes 11 studies, we describe general trends here rather than attempting to conduct statistical tests (which would be dramatically under-powered after correcting for multiple comparisons). Overall, projects had a slightly larger probability of being judged successful if they were judged to be closer to the original (above the median subjective rating; `r 100 * round(mean(d$replicated_TA[d$high_fidelity == TRUE]), digits=2)`\% 
vs. 
`r 100 * round(mean(d$replicated_TA[d$high_fidelity == FALSE]), digits=2)`\%). 
There were also small numerical effects of the original study being run on Mechanical Turk (`r 100 *round(mean(d$replicated_TA[d$original_on_turk == "Yes"]), digits=2)`\% vs. `r 100 *round(mean(d$replicated_TA[d$original_on_turk == "No"]), digits=2)`\%) and having open materials (`r 100 *round(mean(d$replicated_TA[d$open_materials!="No"]), digits=2)`\% 
vs. `r 100 *round(mean(d$replicated_TA[d$open_materials=="No"]), digits=2)`\%), though we do not believe these differences are interpretable given our sample size. 

### Significance of the key effect 

```{r}
d$original_p_value_bins <- .1
d$original_p_value_bins[d$original_p_value %in% c("<.05","0.04")] <- "<.05"
d$original_p_value_bins[d$original_p_value %in% c("0.007","0.001")] <- "<.01"
d$original_p_value_bins[d$original_p_value %in% c("<.001","< .001")] <- "<.001"
d$rep_p_value_05 <- d$rep_p_value < .05

d$replicated_TA_factor <- factor(d$replicated_TA, levels = c(0, .5, 1), 
                                 labels = c("No","Partial","Yes"))
# ggplot(d,aes(x=original_p_value_bins,y=rep_p_value, 
#              color=d$replicated_TA_factor)) +
#   geom_jitter(height=0, width=.2) +
#   geom_hline(yintercept=c(0.05), lty = 2) + 
#   geom_hline(yintercept=c(0.1), lty = 3) + 
#   ylim(c(0,1)) +
#   xlab("Original p-value (binned)") + 
#   ylab("Replication p-value (exact)") + 
#   theme_bw() + 
#   scale_color_solarized(name = "Subjective replication rating") + 
#   theme(legend.position = "bottom")
```

`r sum(d$rep_p_value_05)` of the studies (`r round(mean(d$rep_p_value_05)*100)`%) yielded a significant replication $p$-value at $p < .05$. 
<!-- The well-documented "dance of the p-values" [@cumming2013] suggests that the inference drawn from a single replication $p$-value may not be conclusive, however, and other metrics such as effect sizes and Bayes factors may provide a more nuanced perspective.  -->

### Effect size and Bayes factor analysis

<!-- Effect sizes and Bayes factors supplement inferences drawn from $p$-values in different ways. The effect size is an estimate of the *magnitude* of an effect. Unlike the $p$-value, this underlying magnitude is stable across sample sizes (assuming a constant experimental procedure) though the estimate may be more or less noisy. It thus allows for a more fine-grained comparison across studies than a binary significance criterion. The Bayes Factor is an alternative hypothesis testing method that directly quantifies the evidence in favor of one hypothesis relative to another [@jeffreys1961]. Among other advantages, it does not privilege the null hypothesis and can provide evidence either for or against a hypothesis [see also @scheibehenne2016;@wagenmakers2016].  While the Bayes Factor often agrees with the $p$-value as to which hypothesis is more likely [@wetzels2011], they often substantially disagree on the strength of the evidence. -->

<!-- These additional measures therefore give additional criteria for evaluating replications: If the replication shows roughly the same effect size or Bayes factor as the original results despite $p$-values on different sides of the arbitrary $p = 0.05$ threshold, we may nonetheless consider it more of a success. -->

To compute a standardized effect size, we followed @osc2015 in converting test statistics to a measure of "correlation coefficient per df," which is bounded between 0 and 1.^[For example, the conversion from a $t$ statistic is given by $r = \sqrt{t^2/(t^2 + df)}$, where $df$ is the degrees of freedom. Other formulae are given in @osc2015, Supplemental Information.] While we preregistered an analysis determining whether the original reported effect size is included in the 95\% confidence interval found in the replication, we instead opted for a straightforward comparison of point estimates. This choice was motivated by the lack of consensus on how to conduct such a test [@anderson2016;@gilbert2016], recent concerns that using confidence intervals in this manner may lead to misleading interpretations [@morey2016], and discrepancies in the authors' reporting of effect sizes across the articles we included (which made computing confidence intervals difficult in some cases).

We first compared effect sizes from the original and replication studies (Fig. 2A). Effect sizes were highly correlated between the two sets of studies 
(r = `r signif(cor.test(d$original_ES_d,d$rep_es)$estimate, digits =2)`, p < .0001), but they were generally smaller in the replications 
(`r signif(coef(lm(d$rep_es~d$original_ES_d-1))[1], digits = 2)*100`\% of the original; 95\% CI [`r signif(confint(lm(d$rep_es~d$original_ES_d-1))[1], digits = 2)*100` - `r signif(confint(lm(d$rep_es~d$original_ES_d-1))[2], digits = 2)*100`]), consistent with findings from multi-study replication projects [e.g., @osc2015].

Next, following @etz2016, we compared Bayes Factors between the original and replication. We used the default test suggested by @rouder2009 and did not attempt to correct for publication bias.^[This default test assumes an alternative hypothesis that the effect is present with effect size $d=.707$; a default BF for one paper could not be computed based on the statistical test that was used.] As with effect sizes, we saw generally smaller Bayes Factors for the replications than the originals (Fig. 2B). In addition, it appeared that replication Bayes Factors generally tracked nicely with subjective replication judgments. Finally, the Bayes Factors for several of the original effects did not appear to show strong evidential value (e.g., BF < 3, indicating that the alternative hypothesis was less than three times more likely than the null). 

```{r, fig.cap="Replication effect size (A) and Bayes factor (B), plotted by the original effect size and Bayes factor, respectively. Point size shows replication N (A) and ratio of test degrees of freedom (B), color indicates subjective replication assessments by the authors. Note that the key statistic for one replication was a multivariate $F$ test, hence a comparable default Bayes factor could not be computed. Additionally, the original Bayes factor for one finding was many orders of magnitude greater than the others so it is not displayed.", fig.pos="ht", fig.height=3.6}

# Want semantically meaningful values for coding values
colorMap <- c('No'=rgb(0.75, 0.2, 0.2), 
              'Partial'=rgb(0.8, 0.8, 0.4), 
              'Yes'=rgb(0.2, 0.6, 0.2))

p1 <- ggplot(d,aes(x=original_ES_d,y=rep_es)) +
  geom_point(alpha = .8, pch=21,
             aes(fill=replicated_TA_factor, size = collected_n)) +
  # geom_text_repel(aes(label = first_author_name), size = 2, force = 2) +
  scale_x_continuous(limits = c(0, 1), 
                breaks = c(.1, .3, .5, .7, .9)) +
  scale_y_continuous(limits = c(0, 1), 
                breaks = c(.1, .3, .5, .7, .9)) +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  geom_smooth(method = "lm", se=FALSE, colour="#657b83") +
  xlab("Orig. effect size (r)") + 
  ylab("Rep. effect size (r)") + 
  theme_bw(base_size=10) + theme(legend.position="bottom", aspect.ratio = 1) + 
  scale_size(name = "Rep. N") +
  scale_fill_manual(values = colorMap, guide = FALSE)  +
  #scale_color_solarized(name = "Subj. rep. rating") +
  guides(size=guide_legend(order = 1, direction = "horizontal", byrow=TRUE))


getBF <- function(t_stat, t_df) {
  return(ttest.tstat(t = t_stat, n1 = t_df, rscale = 1, simple = T))
};

tstatDF <- d %>%
  filter(original_t_stat != "NA") %>%
  select(project_key, replicated_TA_factor, first_author_name, original_t_stat, original_t_df, rep_t_stat, rep_t_df) %>%
  rowwise() %>%
  mutate(origBF = getBF(original_t_stat, original_t_df)) %>%
  rowwise() %>%
  mutate(repBF = getBF(rep_t_stat, rep_t_df), 
         df_ratio = rep_t_df/original_t_df)

# exclude atir
tstatDF$origBF[7] <- NA

p2 <- ggplot(tstatDF, aes(x = origBF, y = repBF)) +
  geom_point(alpha = .8, pch=21, aes(size = df_ratio, 
                             fill = replicated_TA_factor)) +
  geom_smooth(method = "lm", se = FALSE, colour="#657b83") +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  xlab("Orig. Bayes Factor") + 
  ylab("Rep. Bayes Factor") + 
  scale_x_log10(limits = c(10**-1.75, 10**11), 
                breaks = c(1/10, 1, 10, 100, 1e+3, 1e+6, 1e+9),
                labels = c(".1", "1", "10","100", "1e3", 
                           "1e6","1e9")) +
  scale_y_log10(limits = c(10**-1.75, 10**11),
                breaks = c(1/10, 1, 10, 100, 1e+3, 1e+6, 1e+9),
                labels = c(".1", "1", "10","100", "1e3", 
                           "1e6","1e9")) + 
  theme_bw(base_size=10) + 
  scale_size_continuous(breaks=c(.5, 2, 5),name = "Rep. df/Orig. df") + 
  scale_fill_manual(values = colorMap, guide = F) +
  theme(legend.position="bottom", aspect.ratio = 1) + 
  guides(size=guide_legend(direction = 'horizontal'))

plotrow <- plot_grid(p1, p2, labels = c("A", "B"))
legend <- get_legend(
  p2 + 
    theme(legend.position="bottom") + 
    guides(fill = guide_legend(title = "Subjective replication rating", 
                                direction = 'horizontal',
                                override.aes = list(size=8)),
           size = FALSE)
)
plot_grid(plotrow, legend, ncol = 1, rel_heights = c(1, .2))
```

## Discussion: Implications for Replication Research

### Statistical Findings

From a purely statistical point of view, the results of our studies were underwhelming. Despite our relatively close adherence to the original protocols and relatively large sample sizes, the modal outcome of our projects was partial replication. These partial replications often were cases in which some hint of the original pattern was observed but the key statistical test was not statistically significant and showed both smaller effect size and lower evidential value than the same test in the original. We invite readers to browse the narrative descriptions of individual replication attempts in our Supplemental Material to see the ways that patterns of empirical findings can differ from one another beyond the significance of a single test.

<!-- Past work has explicitly attempted to estimate the proportion of true effects in a particular literature [@ioannidis2005;@osc2015], but this goal is challenging -- perhaps even impossible [@gilbert2016;@anderson2016]. Presupposing that this construct is even meaningful [cf. @gergen1973], assessing the truth of even an individual effect relies on delineating what constitutes an exact replication [e.g., @stroebe2014] and establishing a standard for replication success [e.g., @simonsohn2015]. Estimating the proportion of true effects in a literature further requires a systematic sampling strategy to ensure generality. Furthermore, no single replication attempt is definitive, so multiple iterations are typically necessary to increase the evidential value of replication attempts [@lewis2016;@klein2014;@wagenmakers2016]. -->

<!-- One potential concern about our project is that our replications might be of lower quality, due to the lead role played by inexperienced investigators. This issue should be taken seriously by any instructor interested in doing replications in the classroom, but we believe it is not a major concern in the current case. First, our experimenters were graduate students, conducting other research in addition to their work in our class. Many of them will go on to publish in top journals (often alongside more senior collaborators), and this work will be treated on its merits, not on the basis of the experience level of the lead author. Indeed, evaluation on merits is even easier in our case, as all materials and methods for each project (including the full experiment) are easily accessible online. Second, the design of our course -- with multiple rounds of review by both graduate student teaching assistants and a faculty member -- mirrors the typical structure of experimental work, where there are multiple sources of feedback from researchers at different career stages. In general, we don't see any reason why the career stage of the students in our class should be more of a concern in the current case than it is for other research. -->

Our statistical findings mask a more optimistic message, however: In most of our projects, the next step for a motivated experimenter is clear. For example, in some projects we suspect that a followup could find strong evidence for the phenomenon of interest by altering the particular planned analysis or titrating the difficulty of the stimulus materials. In others, the difficulty appeared to be statistical power, since differences were in the predicted direction and sometimes reached significance in subsidiary analyses, so a followup would likely require a larger sample. And in some, differences of population would mean that followups would require further stimulus or task adaptation. More generally, we believe that our work here underscores the importance of iterated replication for pinpointing empirical effects and refining theories [see e.g., @lewis2016 for an example of this strategy and some discussion]. 

Many statistical factors leading to lowered replicability have been discussed in past work, including analytic flexibility, context dependency, publication bias, and low statistical power among others [e.g., @button2013; @vanbavel2016; @ioannidis2005]. For both reasons of design and scale, our study here cannot disentangle these factors empirically. Nevertheless, the decreases in effect size and evidential value we report seem consistent with two explanations. First, we likely saw a "winner's curse," with initial publications tending to over-estimate effects. Second, we also might have seen a reduction in effect size because the original studies were tailored to their specific context and population, whereas our replications may not have been [@vanbavel2016; but cf. @inbar2016]. Future work would benefit from a statement of constraints on generality so as to provide a guide to the conditions under which an effect is likely to be present [@simons2016]. 
<!-- Both of these explanations are consistent with our observations, and, based on the previous literature [@osf2015,@vanbavel2016], we speculate that both winner's curse and context effects were present to some degree. -->

### Key Tests

Our experiences performing and compiling the studies here sheds light on one further concern that we believe has been under-reported in past studies [including @osc2015]. Our standard model of replication is based on the notion of a single key statistical test -- and its associated effect size -- being the properties of a study that can be targeted for replication. Yet, as we mentioned above, selecting this "key test" was difficult for most of the projects in our sample, and virtually impossible for some. We were often forced to consult other sections of the papers (e.g., abstracts, discussion sections) to gain clarity on what test was considered the critical one for the authors' interpretation. It is likely that some of our decisions would not be ratified by the original authors. 

Indeed, no study we attempted to replicate was pre-registered (though some were internal replications), and the general pattern of evidence across studies was often more important to the authors' conclusions than any particular test in any single study.  Almost every study conducted multiple statistical tests, often associated with several tersely-reported statistical models with differing specifications. And in some cases, there was no conventionally-reported and easily calculable effect size measure [e.g., for mixed- and random-effect models, cf. @bakeman2005]. This set of features -- which, in our experience, are endemic to published psychological work, including our own -- makes it extremely challenging to do replication research focused on the estimation of a single effect size. 

<!-- In our view, two recommendations will improve this situation. First, authors should specify their "key test" more clearly. Pre-registration might facilitate this step, as regstration typically requires the selection of particular statistical analyses to be designated as critical for interpretation, but we believe this designation is critical even in un-registered experiments. Had "key test" information been available, it would have dramatically simplified the problem of selecting the effect to replicate in several cases. Second, the widespread adoption of open data and open analytic code practices would facilitate replication research. In addition to clarifying the summary report of results in a paper, the availability of data dramatically facilitates the computation of the relevant meta-analytic effect size for power analysis and statistical comparison.^[We note that, although we were grateful that several authors of the papers in our sample shared data and materials, the use of open (non-proprietary) formats for archiving data -- and in particular analytic code -- is often critical for investigators attempting to reproduce analyses.]  -->

# General Discussion

In this paper, we reported a series of 11 student replications of previously-published empirical studies from the 2015 volume of *Psychological Science*. Our goal was to provide a proof of concept for pedagogical replications of recent findings in a top journal. Rather than attempting to gauge the truth of particular effects, our project aimed to assess the challenges of replicating findings -- selected on grounds of feasibility and interest -- within the constraints of a course project. Such classes could become the backbone of future collaborative replication efforts [@frank2012;@everett2015], and could help provide a source of data on the robustness of the empirical literature more generally. 

This vision of collaborative pedagogical replication requires instructors to make class results more broadly discoverable. We can envision a number of possible avenues for this kind of sharing. For example, if some products are shared openly in a repository like the Open Science Framework (osf.io) or figshare (figshare.com) and tagged appropriately, they will be discoverable via search engines. Alternatively, a more structured method for sharing and discovery would be upload to specific replication curation websites like PsychFileDrawer (psychfiledrawer.org) or CurateScience (curatescience.org). Both of these models assume limited coordination across instructors, but more structured collaborations are of course possible. For example, the Collaborative Replications and Education Project (osf.io/wfc6u) selected a subsample of important and feasible studies for replication and helped provide materials to instructors with the explicit goal of encouraging cross-lab pedagogical replications. On these models, instructors must of course curate which projects are released publicly. But by pre-specifying review criteria, instructors can incorporate dissemination into their course as a reward for high quality work. 

<!-- We are generally optimistic about the contributions of student work, but of course not every student project will be scientifically informative. To take an edge case, imagine a student makes an error in stimulus presentation that is not caught by the instructors until after data collection. In this case, we would argue that this work should not be deposited in a targeted replication repository. But in the end, these decisions will be the responsibility of the course instructor -- much like decisions about when to publish other research.   -->

In sum, our results here demonstrate the practical possibility of performing replication research in the classroom. There are many challenges for ensuring high-quality replications in a pedagogical setting -- from checking experimental methods to reviewing analytic code for errors -- but we would argue that these are not just pedagogical challenges. They are challenges for psychological science. We believe that the openness and transparency we pursued here as part of our pedagogical goals should be models not just for other classes but for future research more broadly. 

# Supplemental Material: Project-By-Project Methods Addendum


```{r, fig.height=9, fig.width=6, fig.cap="Side-by-side plots for each attempted replication. Error bars show standard error of the mean. Original data estimated from figures when not otherwise available."}
  # colors:268bd2 859900 dc322f
library(png)
layout(matrix(1:24,nr=6,byr=T))
project_key_sorted <- c("ehermann", "jsalloum", "smraposo", "jedtan", "salehi", "jreynolds","auc","mkeil","jmarias","lampinen","rhiac") #alphabetical by author

for (key in project_key_sorted) {
  orig <- readPNG(paste0("../figures/",key,"-original.png"))
  rep <- readPNG(paste0("../figures/",key,"-replication.png"))

  par(mai=c(0,0,0,0))
  plot(c(0, 100), c(0, 100), type = "n", xlab="", ylab="", axes=FALSE)
  rasterImage(orig, 0, 0, 100, 100, interpolate=FALSE)

  plot(c(0, 100), c(0, 100), type = "n", xlab="", ylab="", axes=FALSE)
  rasterImage(rep, 0, 0, 100, 100, interpolate=FALSE)

}
# Add legend
# legend <- readPNG(paste0("../figures/Replication-Legend.png"))

# plot(c(0, 100), c(0, 100), type = "n", xlab="", ylab="", axes=F)
  # rasterImage(legend, 0, 0, 100, 100, interpolate=FALSE)
```

For each project, we present a summary of the finding, the key statistical test, the impressionistic outcomes of the replication, and -- where applicable -- our assessment of the source of differences in results (often composed with the advice of the original authors). Fig. 3 shows a side-by-side comparison of the key visualization for each study. 

## Atir, Rosenzweig, & Dunning (2015)

This paper investigated how perceived knowledge affects "overclaiming" (claiming knowledge that you do not have). We replicated Study 1b, which asked participants to rate their knowledge about finances, and then asked them to rate their knowledge about a set of financial terms, some of which were invented (and hence for which a positive knowledge statement would necessarily be an overclaim). The original study reported that participants with higher self-rated financial knowledge also overclaimed at a higher rate. The key test statistic for this study was the coefficient in a regression model predicting overclaiming as a function of claimed personal financial knowledge, controlling for accuracy in the true financial knowledge questions. We successfully replicated this effect, despite our use of a much smaller sample (due to the extremely high post-hoc power of the original study). 

## Ko, Sadler, & Galinksy (2015)

This paper investigated vocal cues to social hierarchy. Our target for replication was Experiment 2, which assessed whether participants used particular acoustic cues to make inferences about speakers' level of social hierarchy, particularly that participants from the high-rank condition in Experiment 1 were rated as more highly ranking. This paper employed a two-stage procedure in which participant-generated materials were rated by an independent sample. However -- in contrast to the study of Scopelliti, Lowenstein & Vosgerau (2015), see below -- because materials were collected in Experiment 1 (and were available for reuse), we elected to replicate only Experiment 2, the judgment study. The key test statistic was the main effect of hierarchy condition on "behavior score" (an index of whether speakers would engage in high-status behaviors). The authors were also interested in which particular vocal cues predicted participants judgments, but we judged these analyses to be more descriptive and exploratory. We successfully replicated the main effect of condition on hierarchy judgment, and additionally found a comparable main effect of speaker gender. 

## Lewis & Oyserman (2015)

This paper investigated motivations to save for retirement, and specifically whether changing participants' relationships to the future (making it feel more imminent) would lead them to report that they would begin saving for retirement sooner. We replicated Study 4, which found that seeing the time to retirement in days (10,950) rather than years (30) caused participants to say they would begin saving sooner. The original study reported this analysis both with and without demographic controls. We chose as our key analysis the version of this regression which controlled for age, education, and income (there was also a manipulation of whether saving was incremental that did not result in an effect). 

We failed to find a significant relationship between the time metric manipulation and saving time when including demographic controls. However, when we did not include demographic controls, we found a marginally-significant relationship. In exploratory analyses, we also observed an unpredicted effect of income such that participants who reported higher income selected to start saving sooner. 

## Liverence & Scholl (2015)

This paper tested the theory that persistent object representations could assist in spatial navigation, using a navigation paradigm in which participants used key presses to move though a grid of pictures that were visible only one at a time. Experiment 1, our target for replication, found that participants located targets faster when navigation involved persistence cues (via sliding animations) than when persistence was disrupted (via temporally-matched fading animations). The original authors did not report training effects either within or across testing epochs (groups of 50 trials), so in our adaptation we decreased the length of the paradigm from four to two epochs.

The key test for our replication was a simple *t*-test comparing speed to find the target in the two conditions (slide vs. fade), not controlling for learning across the duration of the experiment. We failed to find this overall difference. In an exploratory followup analysis using a linear mixed effects model, however, we did find a highly-significant effect of condition (when controlling for learning epoch). Our data may have been more variable for a number of reasons: unlike in the original paradigm, our participants were not required to click when they found targets; our stimuli differed slightly (the images were different and featured thin black borders); and our participants differed in age and educational status from the undergraduates in the original study.

## Proudfoot, Kay, & Koval (2015)

This paper examined whether judgments of creativity are influenced by the gender of the creator. In Study 1, our replication target, they tested the hypothesis that "out of the box" creativity was more associated with masculine traits than feminine traits by manipulating the definition of creativity that participants saw (convergent vs. divergent) and asking participants to judge the centrality of different traits to that definition. The original pre-registration of our replication selected a secondary analysis as the key test statistic (an ANOVA over two of the four cells), but in post-data collection review the instructor team decided that a test of the interaction between trait gender and definition condition was closer to the central theoretical claim of the paper. We use this latter test as the key test statistic. Our replication study found an significant interaction of similar magnitude.
 
## Scopelliti, Lowenstein & Vosgerau (2015)

The goal of this paper was to examine whether people are accurately calibrated in their estimates of how others will respond to their attempts at self-promotion. Our replication target was Experiment 3, which showed that participants, who were instructed to describe themselves in such a way that others would like them, expected to be liked more -- but were actually liked less -- than participants who were asked to describe themselves but not additionally instructed to maximize others’ interest in meeting them. The authors performed a series of regressions on four dependent variables capturing judgments about profile writers (interest, liking, bragging, and success), analyzing these as a function of condition (control vs. "maximize interest") and who the evaluator was (the writer vs. an independent sample). Power analysis for this design was complex because the procedure had two stages: In a first stage participants generated descriptions and predicted judgments, while in a second stage a separate sample rated the descriptions. 

We were faced with a choice: Either we could have re-rated the descriptions gathered in the original study (by contacting the author and treating these as "materials" for the study), effectively replicating only stage two, or we could redo the entire two-step procedure. We elected to conduct a replication of the full procedure. In our replication, to decrease cost we focused on the liking judgments, which appeared to be central to the authors' conclusions. Our key test was the interaction of rater (writer vs. independent) and condition.^[We note that this project was submitted late and the student failed to preregister this analysis formally; the pre-written analysis protocol went through pre-data collection instructor review just as with other projects, however.]  However, to power even this smaller study adequately was outside of our budget, so instead of having each judge rate a subset of 10 out of 100 total profiles, we collected a smaller sample of profiles (18) and had every profile rated by each judge. 

We failed to find the predicted interaction in this new sample. Consistent with the original report, though, we found a main effect such that stage one participants predicted that raters would like them more than the stage two participants actually did. We may have suffered from low power due to the relatively small number of profiles we collected in the replication study. In addition, a small number of our profile writers produced language in a manipulation check suggesting that the "maximize interest" manipulation had not succeeded fully; since there was no specified exclusion criterion in the original study, we included these participants. 

## Sofer et al. (2015)

This paper tested the hypothesis that face typicality affects trustworthiness judgments such that the most typical face will be judged to be the most trustworthy. In Study 1, our replication target, faces of varying typicality were rated on their trustworthiness and attractiveness; while trustworthiness peaked at the most typical face, less typical faces were judged more attractive. The original sample was Israeli women, and typical faces were constructed to be typical for that sample. Rather than attempting to construct a typical face for AMT workers, we instead used the stimuli provided by the original authors, recognizing that this decision limited the interpretation of our findings. 

While the original course project selected a global, item-wise regression model as the key statistical test of interest, the instructor team (post-data collection) determined that the fit of this model did not in fact correspond most closely with the authors' stated hypothesis. We thus selected the interaction between face typicality and rating condition as our key test statistic and used the by-participant regression model (which the original paper also reported subsequent to the by-items model) as the target model. Our replication study successfully showed this same interaction. We note that, despite the significant interaction, in our study trustworthiness judgments did not change substantially with typicality (though attractiveness did) and trustworthiness did not peak numerically at the most typical face. This result was plausibly caused by the fact that the "typical" faces were probably not typical for many of our AMT participants (who were of both genders and likely from a diverse range of ethnic backgrounds). 

## Storm & Stone (2015)

This paper found that when participants had the opportunity to save a file containing a list of target words before studying another file, they retained more information from the second study session. Our target for replication was Experiment 3, in which the effect was found to depend on the amount of information that was studied in the first file, with a greater effect for an eight-word list than a two-word list. The key statistical test we selected was the interaction in a 2x2 mixed-design ANOVA (save vs. no-save condition and two- vs. eight-word condition). The replication study failed to find evidence of the interaction. There was a main effect of save condition, however, hence the authors' key manipulation succeeded for both load conditions. Perhaps even two words was sufficient to create interference for our online sample.

## Wang et al. (2015)

This paper reported that the relationship between math anxiety and math performance follows an inverted-U pattern among students with higher intrinsic math motivation, whereas this same relationship is linearly negative among students with lower intrinsic math motivation. Our replication target was Study 2, the authors' own replication study, in which college undergraduates filled out three surveys about their math motivation and anxiety and completed a math task to measure performance. The key effect we targeted was the interaction of math anxiety squared and math motivation in predicting performance. We failed to find evidence for this effect. 

In contrast to the original study, we found a simple linear trend such that performance decreased with increasing math anxiety for both high and low math-motivation groups. Population differences provide one plausible explanation for the differences we found (especially since the original study was itself a replication): Math performance was overall lower in our AMT sample than in the original undergraduate sample. It is possible that either the inverted U was undetectable given this lower level of performance, or that levels of motivation differed enough that the posited relationship did not hold. 

## Xu & Franconeri (2015)

This paper explored constraints on visual working memory in the context of mental rotation. In Experiment 1a, our replication target, participants performed a verbal suppression task to control for use of verbal encoding while attempting to remember a cross with four colored bars, which sometimes switched colors; performing mental rotation significantly impaired ability to detect these bar swaps. To port the task to an online framework we decreased the number of trials and increased the number of participants we tested. The key test statistic was a comparison of $K$ (a metric of the number of visual features remembered) between the rotation and no-rotation conditions. We observed a marginally-significant effect in our replication study. Our study showed a floor effect on capacity such that participants remembered on average very little; it is possible that small details in the displays or the differing AMT participant population led to this floor effect. 

## Zaval, Markowitz, & Weber (2015)

This paper investigated whether encouraging participants to think about their personal legacy could increase concern for the environment. The main study of the paper investigated whether a legacy priming writing exercise would increase a variety of environmental measures, including donations to an environmental charity, pro-environmental intentions, and climate change beliefs. The key test we selected was the effect of condition (priming vs. control) on behavioral intentions in an analysis of variance. We observed a marginally significant effect in the same direction in our replication, as well as support for the mediating relationship of legacy motives on behavioral intentions. Our study may have been under-powered due to budgetary constraints; we ran a sample of comparable size to the original sample of 312 participants. We also may have observed a smaller effect size due to the relatively smaller amount of time our participants spent on the legacy prime writing exercise. We enforced a four minute writing time and a 20 word minimum; we found that most participants stayed relatively closer to these limits than participants in the original study, leading to an overall shorter priming phase with less writing. 

# References 
